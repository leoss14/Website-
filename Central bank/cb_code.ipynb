{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55e4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "REINFORCEMENT LEARNING FOR MONETARY POLICY\n",
      "======================================================================\n",
      "Output path: /Users/leoss/Desktop/Portfolio/Website-/Central bank/Outputs\n",
      "======================================================================\n",
      "LOADING DATA\n",
      "======================================================================\n",
      "✓ CPI: 948 monthly obs\n",
      "✓ Unemployment: 936 monthly obs\n",
      "✓ Fed Funds: 858 monthly obs\n",
      "✓ Real GDP: 315 quarterly obs\n",
      "\n",
      "✓ Merged quarterly data: 285 observations\n",
      "  Period: 1954-07 to 2025-07\n",
      "\n",
      "======================================================================\n",
      "CREATING FEATURES\n",
      "======================================================================\n",
      "✓ Created 2 lags + change features\n",
      "✓ Final dataset: 283 observations\n",
      "\n",
      "======================================================================\n",
      "TRAINING ECONOMY MODELS\n",
      "======================================================================\n",
      "Train: 226 obs | Test: 57 obs\n",
      "\n",
      "✓ Output Gap Model (best arch: (64, 32)):\n",
      "  Test MSE: 1.8622 | R²: 0.7423\n",
      "\n",
      "✓ Inflation Model (best arch: (32, 16)):\n",
      "  Test MSE: 0.9391 | R²: 0.7620\n",
      "\n",
      "======================================================================\n",
      "CREATING ENVIRONMENT\n",
      "======================================================================\n",
      "✓ Environment created (Actions: 17)\n",
      "\n",
      "======================================================================\n",
      "BASELINE EVALUATION\n",
      "======================================================================\n",
      "Taylor Rule:     -194.60 ± 167.04\n",
      "Inertial Taylor: -109.72 ± 115.93\n",
      "\n",
      "======================================================================\n",
      "TRAINING RL AGENT\n",
      "======================================================================\n",
      "Ep  100 | Avg Reward:  -282.02 | ε: 0.819 | π: 3.8% | y: -1.6%\n",
      "Ep  200 | Avg Reward:  -250.62 | ε: 0.670 | π: 3.6% | y: -1.3%\n",
      "Ep  300 | Avg Reward:  -227.09 | ε: 0.548 | π: 3.4% | y: -0.8%\n",
      "Ep  400 | Avg Reward:  -179.41 | ε: 0.449 | π: 3.7% | y: -1.6%\n",
      "Ep  500 | Avg Reward:  -140.05 | ε: 0.368 | π: 2.8% | y: -0.5%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reinforcement Learning for Monetary Policy Optimization (v2)\n",
    "============================================================\n",
    "Complete implementation with visualizations\n",
    "\n",
    "Authors: Leonardo Luksic, Krisha Chandnani, Ignacio Orueta\n",
    "LSE - February 2026\n",
    "\n",
    "OUTPUT PATH: /Users/leoss/Desktop/Portfolio/Website-/Central bank/Outputs\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import copy\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "DATA_PATH = '/Users/leoss/Downloads'\n",
    "OUTPUT_PATH = '/Users/leoss/Desktop/Portfolio/Website-/Central bank/Outputs'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. DATA LOADING AND PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def load_and_prepare_data(data_path):\n",
    "    \"\"\"Load FRED data and convert to quarterly frequency.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    cpi = pd.read_csv(f'{data_path}/CPIAUCSL.csv', parse_dates=['observation_date'], index_col='observation_date')\n",
    "    unrate = pd.read_csv(f'{data_path}/UNRATE.csv', parse_dates=['observation_date'], index_col='observation_date')\n",
    "    fedfunds = pd.read_csv(f'{data_path}/FEDFUNDS-1.csv', parse_dates=['observation_date'], index_col='observation_date')\n",
    "    gdp = pd.read_csv(f'{data_path}/GDPC1-1.csv', parse_dates=['observation_date'], index_col='observation_date')\n",
    "    \n",
    "    print(f\"✓ CPI: {len(cpi)} monthly obs\")\n",
    "    print(f\"✓ Unemployment: {len(unrate)} monthly obs\")\n",
    "    print(f\"✓ Fed Funds: {len(fedfunds)} monthly obs\")\n",
    "    print(f\"✓ Real GDP: {len(gdp)} quarterly obs\")\n",
    "    \n",
    "    cpi_q = cpi.resample('QS').first()\n",
    "    unrate_q = unrate.resample('QS').mean()\n",
    "    fedfunds_q = fedfunds.resample('QS').mean()\n",
    "    \n",
    "    cpi_q['inflation'] = cpi_q['CPIAUCSL'].pct_change(4) * 100\n",
    "    \n",
    "    gdp.columns = ['gdp']\n",
    "    gdp['gdp_trend'] = gdp['gdp'].rolling(window=40, min_periods=10, center=True).mean()\n",
    "    gdp['output_gap'] = ((gdp['gdp'] - gdp['gdp_trend']) / gdp['gdp_trend']) * 100\n",
    "    \n",
    "    data = pd.DataFrame(index=gdp.index)\n",
    "    data['output_gap'] = gdp['output_gap']\n",
    "    data['inflation'] = cpi_q['inflation']\n",
    "    data['unemployment'] = unrate_q['UNRATE']\n",
    "    data['fed_rate'] = fedfunds_q['FEDFUNDS']\n",
    "    data = data.dropna()\n",
    "    \n",
    "    print(f\"\\n✓ Merged quarterly data: {len(data)} observations\")\n",
    "    print(f\"  Period: {data.index.min().strftime('%Y-%m')} to {data.index.max().strftime('%Y-%m')}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_features(data, n_lags=2):\n",
    "    \"\"\"Create lagged features.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREATING FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    for lag in range(1, n_lags + 1):\n",
    "        df[f'L{lag}_inflation'] = df['inflation'].shift(lag)\n",
    "        df[f'L{lag}_output_gap'] = df['output_gap'].shift(lag)\n",
    "        df[f'L{lag}_fed_rate'] = df['fed_rate'].shift(lag)\n",
    "    \n",
    "    df['inflation_change'] = df['inflation'] - df['L1_inflation']\n",
    "    df['output_gap_change'] = df['output_gap'] - df['L1_output_gap']\n",
    "    df['rate_change'] = df['fed_rate'] - df['L1_fed_rate']\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"✓ Created {n_lags} lags + change features\")\n",
    "    print(f\"✓ Final dataset: {len(df)} observations\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ECONOMY MODELS\n",
    "# ============================================================================\n",
    "\n",
    "def train_economy_models(df, test_ratio=0.2):\n",
    "    \"\"\"Train economy models with architecture search.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING ECONOMY MODELS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    split_idx = int(len(df) * (1 - test_ratio))\n",
    "    train = df.iloc[:split_idx]\n",
    "    test = df.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"Train: {len(train)} obs | Test: {len(test)} obs\")\n",
    "    \n",
    "    features_y = ['L1_output_gap', 'L2_output_gap', 'L1_inflation', 'L2_inflation', \n",
    "                  'L1_fed_rate', 'L2_fed_rate']\n",
    "    \n",
    "    X_train_y, y_train_y = train[features_y], train['output_gap']\n",
    "    X_test_y, y_test_y = test[features_y], test['output_gap']\n",
    "    \n",
    "    scaler_y = StandardScaler()\n",
    "    X_train_y_scaled = scaler_y.fit_transform(X_train_y)\n",
    "    X_test_y_scaled = scaler_y.transform(X_test_y)\n",
    "    \n",
    "    best_model_y, best_r2_y = None, -np.inf\n",
    "    architectures = [(32, 16), (64, 32), (64, 32, 16), (128, 64)]\n",
    "    \n",
    "    for arch in architectures:\n",
    "        model = MLPRegressor(\n",
    "            hidden_layer_sizes=arch, activation='relu', solver='adam',\n",
    "            alpha=0.01, max_iter=1500, early_stopping=True,\n",
    "            validation_fraction=0.15, n_iter_no_change=20,\n",
    "            random_state=42, verbose=False\n",
    "        )\n",
    "        model.fit(X_train_y_scaled, y_train_y)\n",
    "        r2 = r2_score(y_test_y, model.predict(X_test_y_scaled))\n",
    "        if r2 > best_r2_y:\n",
    "            best_r2_y = r2\n",
    "            best_model_y = model\n",
    "    \n",
    "    y_pred_y = best_model_y.predict(X_test_y_scaled)\n",
    "    mse_y = mean_squared_error(y_test_y, y_pred_y)\n",
    "    \n",
    "    print(f\"\\n✓ Output Gap Model (best arch: {best_model_y.hidden_layer_sizes}):\")\n",
    "    print(f\"  Test MSE: {mse_y:.4f} | R²: {best_r2_y:.4f}\")\n",
    "    \n",
    "    features_pi = ['output_gap', 'L1_output_gap', 'L1_inflation', 'L2_inflation',\n",
    "                   'L1_fed_rate', 'L2_fed_rate']\n",
    "    \n",
    "    X_train_pi, y_train_pi = train[features_pi], train['inflation']\n",
    "    X_test_pi, y_test_pi = test[features_pi], test['inflation']\n",
    "    \n",
    "    scaler_pi = StandardScaler()\n",
    "    X_train_pi_scaled = scaler_pi.fit_transform(X_train_pi)\n",
    "    X_test_pi_scaled = scaler_pi.transform(X_test_pi)\n",
    "    \n",
    "    best_model_pi, best_r2_pi = None, -np.inf\n",
    "    \n",
    "    for arch in architectures:\n",
    "        model = MLPRegressor(\n",
    "            hidden_layer_sizes=arch, activation='relu', solver='adam',\n",
    "            alpha=0.01, max_iter=1500, early_stopping=True,\n",
    "            validation_fraction=0.15, n_iter_no_change=20,\n",
    "            random_state=42, verbose=False\n",
    "        )\n",
    "        model.fit(X_train_pi_scaled, y_train_pi)\n",
    "        r2 = r2_score(y_test_pi, model.predict(X_test_pi_scaled))\n",
    "        if r2 > best_r2_pi:\n",
    "            best_r2_pi = r2\n",
    "            best_model_pi = model\n",
    "    \n",
    "    y_pred_pi = best_model_pi.predict(X_test_pi_scaled)\n",
    "    mse_pi = mean_squared_error(y_test_pi, y_pred_pi)\n",
    "    \n",
    "    print(f\"\\n✓ Inflation Model (best arch: {best_model_pi.hidden_layer_sizes}):\")\n",
    "    print(f\"  Test MSE: {mse_pi:.4f} | R²: {best_r2_pi:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model_y': best_model_y, 'model_pi': best_model_pi,\n",
    "        'scaler_y': scaler_y, 'scaler_pi': scaler_pi,\n",
    "        'features_y': features_y, 'features_pi': features_pi,\n",
    "        'metrics': {'y_r2': best_r2_y, 'y_mse': mse_y, 'pi_r2': best_r2_pi, 'pi_mse': mse_pi}\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ENVIRONMENT\n",
    "# ============================================================================\n",
    "\n",
    "class EconomyEnv:\n",
    "    \"\"\"Simulated economy environment for RL.\"\"\"\n",
    "    \n",
    "    def __init__(self, models, historical_df, \n",
    "                 inflation_target=2.0, output_gap_target=0.0,\n",
    "                 omega_pi=0.5, omega_y=0.5, omega_smooth=0.05,\n",
    "                 max_steps=40, n_actions=17, normalize_states=True):\n",
    "        \n",
    "        self.model_y = models['model_y']\n",
    "        self.model_pi = models['model_pi']\n",
    "        self.scaler_y = models['scaler_y']\n",
    "        self.scaler_pi = models['scaler_pi']\n",
    "        self.features_y = models['features_y']\n",
    "        self.features_pi = models['features_pi']\n",
    "        \n",
    "        self.historical_df = historical_df.reset_index(drop=True)\n",
    "        \n",
    "        self.inflation_target = inflation_target\n",
    "        self.output_gap_target = output_gap_target\n",
    "        self.omega_pi = omega_pi\n",
    "        self.omega_y = omega_y\n",
    "        self.omega_smooth = omega_smooth\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.rate_values = np.linspace(0, 16, n_actions)\n",
    "        \n",
    "        self.normalize_states = normalize_states\n",
    "        self.state_dim = 6\n",
    "        \n",
    "        if normalize_states:\n",
    "            self._compute_state_stats()\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _compute_state_stats(self):\n",
    "        states = []\n",
    "        for i in range(2, len(self.historical_df)):\n",
    "            row_t1 = self.historical_df.iloc[i]\n",
    "            row_t2 = self.historical_df.iloc[i-1]\n",
    "            states.append([\n",
    "                row_t1['inflation'], row_t2['inflation'],\n",
    "                row_t1['output_gap'], row_t2['output_gap'],\n",
    "                row_t1['fed_rate'], row_t2['fed_rate']\n",
    "            ])\n",
    "        states = np.array(states)\n",
    "        self.state_mean = states.mean(axis=0)\n",
    "        self.state_std = states.std(axis=0) + 1e-8\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        max_start = len(self.historical_df) - 3\n",
    "        start_idx = np.random.randint(2, max(3, max_start))\n",
    "        \n",
    "        row_t1 = self.historical_df.iloc[start_idx]\n",
    "        row_t2 = self.historical_df.iloc[start_idx - 1]\n",
    "        \n",
    "        self.state = {\n",
    "            'pi_t1': row_t1['inflation'], 'pi_t2': row_t2['inflation'],\n",
    "            'y_t1': row_t1['output_gap'], 'y_t2': row_t2['output_gap'],\n",
    "            'i_t1': row_t1['fed_rate'], 'i_t2': row_t2['fed_rate']\n",
    "        }\n",
    "        \n",
    "        self.step_count = 0\n",
    "        self.episode_history = []\n",
    "        self.cumulative_reward = 0\n",
    "        \n",
    "        return self._get_state_array()\n",
    "    \n",
    "    def _get_state_array(self, normalize=None):\n",
    "        raw = np.array([\n",
    "            self.state['pi_t1'], self.state['pi_t2'],\n",
    "            self.state['y_t1'], self.state['y_t2'],\n",
    "            self.state['i_t1'], self.state['i_t2']\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        if (normalize is None and self.normalize_states) or normalize:\n",
    "            return (raw - self.state_mean) / self.state_std\n",
    "        return raw\n",
    "    \n",
    "    def get_raw_state(self):\n",
    "        return self._get_state_array(normalize=False)\n",
    "    \n",
    "    def step(self, action):\n",
    "        rate = self.rate_values[int(action)]\n",
    "        \n",
    "        features_y = np.array([[\n",
    "            self.state['y_t1'], self.state['y_t2'],\n",
    "            self.state['pi_t1'], self.state['pi_t2'],\n",
    "            self.state['i_t1'], self.state['i_t2']\n",
    "        ]])\n",
    "        features_y_scaled = self.scaler_y.transform(features_y)\n",
    "        next_y = float(self.model_y.predict(features_y_scaled)[0])\n",
    "        next_y = np.clip(next_y, -10, 10)\n",
    "        \n",
    "        features_pi = np.array([[\n",
    "            next_y, self.state['y_t1'],\n",
    "            self.state['pi_t1'], self.state['pi_t2'],\n",
    "            self.state['i_t1'], self.state['i_t2']\n",
    "        ]])\n",
    "        features_pi_scaled = self.scaler_pi.transform(features_pi)\n",
    "        next_pi = float(self.model_pi.predict(features_pi_scaled)[0])\n",
    "        next_pi = np.clip(next_pi, -5, 20)\n",
    "        \n",
    "        inflation_loss = (next_pi - self.inflation_target) ** 2\n",
    "        output_loss = (next_y - self.output_gap_target) ** 2\n",
    "        smoothing_loss = (rate - self.state['i_t1']) ** 2\n",
    "        \n",
    "        reward = -(\n",
    "            self.omega_pi * inflation_loss +\n",
    "            self.omega_y * output_loss +\n",
    "            self.omega_smooth * smoothing_loss\n",
    "        )\n",
    "        \n",
    "        self.episode_history.append({\n",
    "            'inflation': next_pi, 'output_gap': next_y,\n",
    "            'rate': rate, 'reward': reward\n",
    "        })\n",
    "        \n",
    "        self.state = {\n",
    "            'pi_t1': next_pi, 'pi_t2': self.state['pi_t1'],\n",
    "            'y_t1': next_y, 'y_t2': self.state['y_t1'],\n",
    "            'i_t1': rate, 'i_t2': self.state['i_t1']\n",
    "        }\n",
    "        \n",
    "        self.step_count += 1\n",
    "        self.cumulative_reward += reward\n",
    "        done = self.step_count >= self.max_steps\n",
    "        \n",
    "        info = {'inflation': next_pi, 'output_gap': next_y, 'rate': rate}\n",
    "        \n",
    "        return self._get_state_array(), reward, done, info\n",
    "    \n",
    "    def get_episode_stats(self):\n",
    "        if not self.episode_history:\n",
    "            return {}\n",
    "        hist = self.episode_history\n",
    "        return {\n",
    "            'mean_inflation': np.mean([h['inflation'] for h in hist]),\n",
    "            'mean_output_gap': np.mean([h['output_gap'] for h in hist]),\n",
    "            'mean_rate': np.mean([h['rate'] for h in hist]),\n",
    "            'total_reward': self.cumulative_reward\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Q-LEARNING AGENT\n",
    "# ============================================================================\n",
    "\n",
    "class ImprovedQLearningAgent:\n",
    "    def __init__(self, state_dim, n_actions, gamma=0.95,\n",
    "                 epsilon_start=1.0, epsilon_end=0.10, epsilon_decay=0.998,\n",
    "                 buffer_size=10000, batch_size=128, target_update_freq=20):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler_fitted = False\n",
    "        \n",
    "        self.q_network = MLPRegressor(\n",
    "            hidden_layer_sizes=(128, 64, 32), activation='relu',\n",
    "            solver='adam', alpha=0.001, max_iter=1, warm_start=True,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        self.target_network = None\n",
    "        self.update_count = 0\n",
    "        self.initialized = False\n",
    "        self.min_buffer = 200\n",
    "        \n",
    "    def _encode_state_action(self, state, action):\n",
    "        action_onehot = np.zeros(self.n_actions)\n",
    "        action_onehot[action] = 1\n",
    "        return np.concatenate([state, action_onehot])\n",
    "    \n",
    "    def _get_all_q_values(self, state, network=None):\n",
    "        if network is None:\n",
    "            network = self.q_network\n",
    "        \n",
    "        if not self.initialized:\n",
    "            return np.zeros(self.n_actions)\n",
    "        \n",
    "        q_values = []\n",
    "        for a in range(self.n_actions):\n",
    "            sa = self._encode_state_action(state, a).reshape(1, -1)\n",
    "            sa_scaled = self.scaler.transform(sa)\n",
    "            q_values.append(network.predict(sa_scaled)[0])\n",
    "        \n",
    "        return np.array(q_values)\n",
    "    \n",
    "    def choose_action(self, state, greedy=False):\n",
    "        if not greedy and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        q_values = self._get_all_q_values(state)\n",
    "        return int(np.argmax(q_values))\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state.copy(), action, reward, next_state.copy(), done))\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.buffer) < self.min_buffer:\n",
    "            return None\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), self.batch_size, replace=False)\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "        \n",
    "        X, y = [], []\n",
    "        \n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            sa = self._encode_state_action(state, action)\n",
    "            X.append(sa)\n",
    "            \n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                if self.target_network is not None:\n",
    "                    next_q = self._get_all_q_values(next_state, self.target_network)\n",
    "                else:\n",
    "                    next_q = self._get_all_q_values(next_state)\n",
    "                target = reward + self.gamma * np.max(next_q)\n",
    "            \n",
    "            y.append(target)\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        if not self.scaler_fitted:\n",
    "            self.scaler.fit(X)\n",
    "            self.scaler_fitted = True\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            self.q_network.fit(X_scaled, y)\n",
    "            self.initialized = True\n",
    "            self._update_target_network()\n",
    "        else:\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            self.q_network.fit(X_scaled, y)\n",
    "        \n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.target_update_freq == 0:\n",
    "            self._update_target_network()\n",
    "        \n",
    "        y_pred = self.q_network.predict(X_scaled)\n",
    "        loss = np.mean((y - y_pred) ** 2)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _update_target_network(self):\n",
    "        self.target_network = copy.deepcopy(self.q_network)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TAYLOR RULE\n",
    "# ============================================================================\n",
    "\n",
    "def taylor_rule(inflation, output_gap, r_star=2.0, pi_star=2.0, alpha_pi=1.5, alpha_y=0.5):\n",
    "    rate = r_star + inflation + alpha_pi * (inflation - pi_star) + alpha_y * output_gap\n",
    "    return np.clip(rate, 0, 16)\n",
    "\n",
    "def taylor_rule_inertial(inflation, output_gap, prev_rate, r_star=2.0, pi_star=2.0,\n",
    "                         alpha_pi=1.5, alpha_y=0.5, rho=0.8):\n",
    "    taylor_rate = r_star + inflation + alpha_pi * (inflation - pi_star) + alpha_y * output_gap\n",
    "    rate = rho * prev_rate + (1 - rho) * taylor_rate\n",
    "    return np.clip(rate, 0, 16)\n",
    "\n",
    "def evaluate_policy(env, policy_fn, n_episodes=50, policy_name=\"Policy\"):\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = policy_fn(state, env)\n",
    "            state, _, done, _ = env.step(action)\n",
    "        \n",
    "        episode_rewards.append(env.cumulative_reward)\n",
    "    \n",
    "    return {\n",
    "        'name': policy_name,\n",
    "        'mean_reward': np.mean(episode_rewards),\n",
    "        'std_reward': np.std(episode_rewards),\n",
    "        'rewards': episode_rewards\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def train_agent(env, agent, n_episodes=1500, print_every=100):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING RL AGENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_losses = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "        \n",
    "        loss = agent.update()\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        episode_rewards.append(env.cumulative_reward)\n",
    "        episode_losses.append(loss if loss else 0)\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            recent_avg = np.mean(episode_rewards[-print_every:])\n",
    "            stats = env.get_episode_stats()\n",
    "            print(f\"Ep {episode+1:4d} | Avg Reward: {recent_avg:8.2f} | \"\n",
    "                  f\"ε: {agent.epsilon:.3f} | \"\n",
    "                  f\"π: {stats['mean_inflation']:.1f}% | y: {stats['mean_output_gap']:.1f}%\")\n",
    "    \n",
    "    return episode_rewards, episode_losses\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training(episode_rewards, episode_losses, save_path=None):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    window = 20\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(episode_rewards, alpha=0.3, color='steelblue')\n",
    "    if len(episode_rewards) >= window:\n",
    "        smooth = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "        ax1.plot(range(window-1, len(episode_rewards)), smooth,\n",
    "                color='steelblue', linewidth=2, label=f'{window}-ep moving avg')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Episode Reward')\n",
    "    ax1.set_title('Training Rewards')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    valid_losses = [l for l in episode_losses if l and l > 0]\n",
    "    if valid_losses:\n",
    "        ax2.plot(valid_losses, color='coral', alpha=0.5)\n",
    "        if len(valid_losses) >= window:\n",
    "            smooth_loss = np.convolve(valid_losses, np.ones(window)/window, mode='valid')\n",
    "            ax2.plot(range(window-1, len(valid_losses)), smooth_loss, color='coral', linewidth=2)\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Q-Learning Loss')\n",
    "    ax2.set_title('Training Loss')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"✓ Saved {save_path}\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_policy_comparison(env, agent, n_steps=40, seed=42, save_path=None):\n",
    "    def rl_policy(state, env):\n",
    "        return agent.choose_action(state, greedy=True)\n",
    "    \n",
    "    def taylor_policy(state, env):\n",
    "        raw = env.get_raw_state()\n",
    "        rate = taylor_rule(raw[0], raw[2])\n",
    "        return np.argmin(np.abs(env.rate_values - rate))\n",
    "    \n",
    "    def taylor_inertial_policy(state, env):\n",
    "        raw = env.get_raw_state()\n",
    "        rate = taylor_rule_inertial(raw[0], raw[2], raw[4])\n",
    "        return np.argmin(np.abs(env.rate_values - rate))\n",
    "    \n",
    "    policies = [\n",
    "        (rl_policy, \"RL Agent\", 'steelblue', '-'),\n",
    "        (taylor_policy, \"Taylor Rule\", 'coral', '--'),\n",
    "        (taylor_inertial_policy, \"Inertial Taylor\", 'forestgreen', ':')\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "    \n",
    "    for policy_fn, name, color, ls in policies:\n",
    "        state = env.reset(seed=seed)\n",
    "        history = {'inflation': [], 'output_gap': [], 'rate': []}\n",
    "        \n",
    "        for _ in range(n_steps):\n",
    "            action = policy_fn(state, env)\n",
    "            state, _, done, info = env.step(action)\n",
    "            history['inflation'].append(info['inflation'])\n",
    "            history['output_gap'].append(info['output_gap'])\n",
    "            history['rate'].append(info['rate'])\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        quarters = range(len(history['inflation']))\n",
    "        axes[0].plot(quarters, history['inflation'], ls, color=color, linewidth=2, label=name)\n",
    "        axes[1].plot(quarters, history['output_gap'], ls, color=color, linewidth=2, label=name)\n",
    "        axes[2].plot(quarters, history['rate'], ls, color=color, linewidth=2, label=name)\n",
    "    \n",
    "    axes[0].axhline(2.0, color='green', linestyle=':', alpha=0.7, label='Target (2%)')\n",
    "    axes[1].axhline(0.0, color='green', linestyle=':', alpha=0.7, label='Target (0%)')\n",
    "    \n",
    "    axes[0].set_ylabel('Inflation (%)')\n",
    "    axes[0].set_title('Policy Comparison: Simulated Economy')\n",
    "    axes[0].legend(loc='upper right')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_ylabel('Output Gap (%)')\n",
    "    axes[1].legend(loc='upper right')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].set_xlabel('Quarters')\n",
    "    axes[2].set_ylabel('Interest Rate (%)')\n",
    "    axes[2].legend(loc='upper right')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"✓ Saved {save_path}\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_reward_distribution(results_list, save_path=None):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    data = [r['rewards'] for r in results_list]\n",
    "    labels = [r['name'] for r in results_list]\n",
    "    \n",
    "    bp = ax.boxplot(data, labels=labels, patch_artist=True)\n",
    "    \n",
    "    colors = ['steelblue', 'coral', 'forestgreen']\n",
    "    for patch, color in zip(bp['boxes'], colors[:len(data)]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    \n",
    "    ax.set_ylabel('Episode Reward')\n",
    "    ax.set_title('Policy Performance Distribution (50 episodes)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    if len(results_list) >= 2:\n",
    "        t_stat, p_val = stats.ttest_ind(results_list[0]['rewards'], results_list[1]['rewards'])\n",
    "        ax.text(0.02, 0.98, f't-test p-value: {p_val:.4f}',\n",
    "               transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"✓ Saved {save_path}\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"REINFORCEMENT LEARNING FOR MONETARY POLICY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Output path: {OUTPUT_PATH}\")\n",
    "    \n",
    "    data = load_and_prepare_data(DATA_PATH)\n",
    "    df = create_features(data)\n",
    "    models = train_economy_models(df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREATING ENVIRONMENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    env = EconomyEnv(\n",
    "        models=models, historical_df=df,\n",
    "        inflation_target=2.0, output_gap_target=0.0,\n",
    "        omega_pi=0.5, omega_y=0.5, omega_smooth=0.05,\n",
    "        max_steps=40, n_actions=17, normalize_states=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Environment created (Actions: {env.n_actions})\")\n",
    "    \n",
    "    agent = ImprovedQLearningAgent(\n",
    "        state_dim=env.state_dim, n_actions=env.n_actions,\n",
    "        gamma=0.95, epsilon_start=1.0, epsilon_end=0.10,\n",
    "        epsilon_decay=0.998, buffer_size=10000, batch_size=128,\n",
    "        target_update_freq=20\n",
    "    )\n",
    "    \n",
    "    def rl_policy(state, env):\n",
    "        return agent.choose_action(state, greedy=True)\n",
    "    \n",
    "    def taylor_policy(state, env):\n",
    "        raw = env.get_raw_state()\n",
    "        rate = taylor_rule(raw[0], raw[2])\n",
    "        return np.argmin(np.abs(env.rate_values - rate))\n",
    "    \n",
    "    def taylor_inertial_policy(state, env):\n",
    "        raw = env.get_raw_state()\n",
    "        rate = taylor_rule_inertial(raw[0], raw[2], raw[4])\n",
    "        return np.argmin(np.abs(env.rate_values - rate))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BASELINE EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    taylor_results = evaluate_policy(env, taylor_policy, n_episodes=50, policy_name=\"Taylor Rule\")\n",
    "    inertial_results = evaluate_policy(env, taylor_inertial_policy, n_episodes=50, policy_name=\"Inertial Taylor\")\n",
    "    \n",
    "    print(f\"Taylor Rule:     {taylor_results['mean_reward']:.2f} ± {taylor_results['std_reward']:.2f}\")\n",
    "    print(f\"Inertial Taylor: {inertial_results['mean_reward']:.2f} ± {inertial_results['std_reward']:.2f}\")\n",
    "    \n",
    "    episode_rewards, episode_losses = train_agent(env, agent, n_episodes=1500, print_every=100)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rl_results = evaluate_policy(env, rl_policy, n_episodes=50, policy_name=\"RL Agent\")\n",
    "    \n",
    "    print(f\"\\nResults (50 evaluation episodes):\")\n",
    "    print(f\"  RL Agent:        {rl_results['mean_reward']:8.2f} ± {rl_results['std_reward']:.2f}\")\n",
    "    print(f\"  Taylor Rule:     {taylor_results['mean_reward']:8.2f} ± {taylor_results['std_reward']:.2f}\")\n",
    "    print(f\"  Inertial Taylor: {inertial_results['mean_reward']:8.2f} ± {inertial_results['std_reward']:.2f}\")\n",
    "    \n",
    "    t_stat, p_val = stats.ttest_ind(rl_results['rewards'], taylor_results['rewards'])\n",
    "    print(f\"\\n  RL vs Taylor t-test: t={t_stat:.2f}, p={p_val:.4f}\")\n",
    "    \n",
    "    if rl_results['mean_reward'] > taylor_results['mean_reward']:\n",
    "        imp = ((rl_results['mean_reward'] - taylor_results['mean_reward']) / \n",
    "               abs(taylor_results['mean_reward'])) * 100\n",
    "        print(f\"\\n✓ RL Agent outperforms Taylor Rule by {imp:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SAVING VISUALIZATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    plot_training(episode_rewards, episode_losses, \n",
    "                  save_path=f'{OUTPUT_PATH}/training_curves.png')\n",
    "    \n",
    "    plot_policy_comparison(env, agent, n_steps=40,\n",
    "                           save_path=f'{OUTPUT_PATH}/policy_comparison.png')\n",
    "    \n",
    "    plot_reward_distribution([rl_results, taylor_results, inertial_results],\n",
    "                             save_path=f'{OUTPUT_PATH}/reward_distribution.png')\n",
    "    \n",
    "    plt.close('all')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return env, agent, models, episode_rewards, rl_results, taylor_results, inertial_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
