{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9abb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n",
      "✓ Output directory: /Users/leoss/Desktop/Portfolio/Website-/UK pink tax/Outputs/charts/ml_pipeline_v3\n",
      "\n",
      "======================================================================\n",
      "LOADING DATA\n",
      "======================================================================\n",
      "✓ Main dataset: 21,436 products\n",
      "✓ Human-coded: 259 products\n",
      "✓ Your labeled: 44 products\n",
      "\n",
      "======================================================================\n",
      "STEP 1: FILTER NON-GENDERED CATEGORIES\n",
      "======================================================================\n",
      "Original: 21,436\n",
      "Excluded: 8,604\n",
      "Remaining: 12,832\n",
      "\n",
      "======================================================================\n",
      "STEP 2: EXTRACT GENDER LABELS\n",
      "======================================================================\n",
      "Label distribution:\n",
      "{'none': 10913, 'female': 1075, 'male': 844}\n",
      "Human labels merged: 200\n",
      "\n",
      "======================================================================\n",
      "STEP 3: COLOR EXTRACTION (POST-FILTERING)\n",
      "======================================================================\n",
      "⚠ No cache found. Extracting colors for filtered products...\n",
      "  This will take a while...\n",
      "  Extracting colors for 2000 products...\n",
      "    Progress: 0/2000 (0.0%)\n",
      "    Progress: 100/2000 (5.0%)\n",
      "    Progress: 200/2000 (10.0%)\n",
      "    Progress: 300/2000 (15.0%)\n",
      "    Progress: 400/2000 (20.0%)\n",
      "    Progress: 500/2000 (25.0%)\n",
      "    Progress: 600/2000 (30.0%)\n",
      "    Progress: 700/2000 (35.0%)\n",
      "    Progress: 800/2000 (40.0%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PINK TAX ANALYSIS: ML-BASED GENDER PREDICTION PIPELINE v3\n",
    "# ============================================================================\n",
    "# \n",
    "# KEY CHANGES FROM v2:\n",
    "# 1. Filter categories FIRST, then extract colors (correct order)\n",
    "# 2. Train primary model on ALL explicitly gendered products (no color requirement)\n",
    "# 3. Color features as optional enhancement, not constraint\n",
    "# 4. Much larger training set (~1,900 vs 87)\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "PATH_MAIN_DATA = '/Users/leoss/Downloads/items_fin.csv'\n",
    "PATH_HUMAN_CODED = '/Users/leoss/Downloads/items_prices_description_gender_humancode_sample.csv'\n",
    "PATH_YOUR_LABELED = '/Users/leoss/Downloads/available_validation.xlsx'\n",
    "OUTPUT_DIR = '/Users/leoss/Desktop/Portfolio/Website-/UK pink tax/Outputs/charts/ml_pipeline_v3'\n",
    "\n",
    "# NEW color cache (post-filtering)\n",
    "COLOR_CACHE_PATH = '/Users/leoss/Desktop/Portfolio/Website-/UK pink tax/Outputs/color_features_cache_v3_filtered.csv'\n",
    "\n",
    "# ML settings\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.25\n",
    "CV_FOLDS = 5\n",
    "\n",
    "# Color extraction\n",
    "N_COLORS = 3\n",
    "COLOR_EXTRACTION_SAMPLE = 2000  # Will extract from FILTERED data\n",
    "\n",
    "# Categories to exclude\n",
    "EXCLUDE_CATEGORIES = [\n",
    "    'food', 'grocery', 'groceries', 'snacks', 'drinks', 'beverages',\n",
    "    'pet food', 'pet supplies', 'cleaning', 'household', 'kitchen',\n",
    "    'office', 'stationery', 'electronics', 'tech', 'garden', 'automotive'\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, classification_report)\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"✓ Imports complete\")\n",
    "print(f\"✓ Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df = pd.read_csv(PATH_MAIN_DATA, encoding='latin-1')\n",
    "df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "if 'unnamed:_0' in df.columns:\n",
    "    df = df.drop(columns=['unnamed:_0'])\n",
    "print(f\"✓ Main dataset: {len(df):,} products\")\n",
    "\n",
    "human_coded = pd.read_csv(PATH_HUMAN_CODED, encoding='latin-1')\n",
    "human_coded.columns = human_coded.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "if 'unnamed:_0' in human_coded.columns:\n",
    "    human_coded = human_coded.drop(columns=['unnamed:_0'])\n",
    "print(f\"✓ Human-coded: {len(human_coded)} products\")\n",
    "\n",
    "try:\n",
    "    your_labeled = pd.read_excel(PATH_YOUR_LABELED)\n",
    "    your_labeled.columns = your_labeled.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "    print(f\"✓ Your labeled: {len(your_labeled)} products\")\n",
    "except:\n",
    "    your_labeled = pd.DataFrame()\n",
    "    print(\"⚠ Your labeled file not found\")\n",
    "\n",
    "# Column mapping\n",
    "COL_NAME = 'product_title_x'\n",
    "COL_DESC = 'description'\n",
    "COL_BREADCRUMB = 'standardized_breadcrumbs'\n",
    "COL_PRICE = 'price'\n",
    "COL_UNIT_PRICE = 'unit_price'\n",
    "COL_IMAGE = 'image_url'\n",
    "COL_STORE = 'store_id'\n",
    "COL_PRODUCT_ID = 'product_id'\n",
    "COL_URL = 'product_url_x'\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: FILTER CATEGORIES FIRST\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: FILTER NON-GENDERED CATEGORIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "original_count = len(df)\n",
    "\n",
    "def contains_excluded_category(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    text_lower = str(text).lower()\n",
    "    return any(cat in text_lower for cat in EXCLUDE_CATEGORIES)\n",
    "\n",
    "df['is_excluded'] = df[COL_BREADCRUMB].apply(contains_excluded_category)\n",
    "excluded_count = df['is_excluded'].sum()\n",
    "\n",
    "print(f\"Original: {original_count:,}\")\n",
    "print(f\"Excluded: {excluded_count:,}\")\n",
    "\n",
    "df = df[~df['is_excluded']].copy().reset_index(drop=True)\n",
    "print(f\"Remaining: {len(df):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: EXTRACT GENDER LABELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: EXTRACT GENDER LABELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "FEMALE_KEYWORDS = ['women', 'woman', 'female', 'ladies', 'lady', 'girls', \n",
    "                   'womens', \"women's\", 'femme', 'her', 'feminine', 'fem']\n",
    "MALE_KEYWORDS = ['men', 'man', 'male', 'gentleman', 'gentlemen', 'boys', \n",
    "                 'mens', \"men's\", 'homme', 'his', 'masculine']\n",
    "ALL_GENDER_KEYWORDS = set(FEMALE_KEYWORDS + MALE_KEYWORDS)\n",
    "\n",
    "def extract_gender_explicit(text):\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return 'none'\n",
    "    text_lower = str(text).lower()\n",
    "    has_female = any(re.search(r'\\b' + kw + r'\\b', text_lower) for kw in FEMALE_KEYWORDS)\n",
    "    has_male = any(re.search(r'\\b' + kw + r'\\b', text_lower) for kw in MALE_KEYWORDS)\n",
    "    \n",
    "    if has_female and not has_male:\n",
    "        return 'female'\n",
    "    elif has_male and not has_female:\n",
    "        return 'male'\n",
    "    elif has_female and has_male:\n",
    "        return 'both'\n",
    "    else:\n",
    "        return 'none'\n",
    "\n",
    "df['label_bc'] = df[COL_BREADCRUMB].apply(extract_gender_explicit)\n",
    "df['label_name'] = df[COL_NAME].apply(extract_gender_explicit)\n",
    "df['label_desc'] = df[COL_DESC].apply(extract_gender_explicit)\n",
    "\n",
    "def combine_labels(row):\n",
    "    for col in ['label_bc', 'label_name', 'label_desc']:\n",
    "        if row[col] in ['female', 'male']:\n",
    "            return row[col]\n",
    "    return 'none'\n",
    "\n",
    "df['label_extracted'] = df.apply(combine_labels, axis=1)\n",
    "\n",
    "print(f\"Label distribution:\")\n",
    "print(df['label_extracted'].value_counts().to_dict())\n",
    "\n",
    "# Merge human labels\n",
    "if 'human_gender_label' in human_coded.columns and COL_PRODUCT_ID in human_coded.columns:\n",
    "    human_labels = human_coded[[COL_PRODUCT_ID, 'human_gender_label']].drop_duplicates()\n",
    "    human_labels.columns = [COL_PRODUCT_ID, 'label_human']\n",
    "    human_labels['label_human'] = human_labels['label_human'].str.lower().str.strip()\n",
    "    df = df.merge(human_labels, on=COL_PRODUCT_ID, how='left')\n",
    "else:\n",
    "    df['label_human'] = None\n",
    "\n",
    "human_count = df['label_human'].notna().sum()\n",
    "print(f\"Human labels merged: {human_count}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: COLOR EXTRACTION (ON FILTERED DATA)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: COLOR EXTRACTION (POST-FILTERING)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "STANDARD_COLORS = {\n",
    "    'dark_red': (139, 0, 0), 'red': (255, 0, 0), 'coral': (255, 127, 80),\n",
    "    'salmon': (250, 128, 114), 'crimson': (220, 20, 60), 'brown': (139, 69, 19),\n",
    "    'tan': (210, 180, 140), 'orange': (255, 165, 0), 'gold': (255, 215, 0),\n",
    "    'yellow': (255, 255, 0), 'khaki': (240, 230, 140), 'dark_green': (0, 100, 0),\n",
    "    'green': (0, 128, 0), 'lime': (50, 205, 50), 'olive': (128, 128, 0),\n",
    "    'teal': (0, 128, 128), 'navy': (0, 0, 128), 'blue': (0, 0, 255),\n",
    "    'royal_blue': (65, 105, 225), 'sky_blue': (135, 206, 235), 'cyan': (0, 255, 255),\n",
    "    'purple': (128, 0, 128), 'magenta': (255, 0, 255), 'violet': (238, 130, 238),\n",
    "    'lavender': (230, 230, 250), 'pink': (255, 192, 203), 'hot_pink': (255, 105, 180),\n",
    "    'gray': (128, 128, 128), 'silver': (192, 192, 192), 'black': (0, 0, 0), 'white': (255, 255, 255)\n",
    "}\n",
    "\n",
    "def color_distance(c1, c2):\n",
    "    return np.sqrt(sum((a - b) ** 2 for a, b in zip(c1, c2)))\n",
    "\n",
    "def closest_standard_color(rgb):\n",
    "    min_dist = float('inf')\n",
    "    closest = 'gray'\n",
    "    for name, std_rgb in STANDARD_COLORS.items():\n",
    "        dist = color_distance(rgb, std_rgb)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest = name\n",
    "    return closest\n",
    "\n",
    "def is_background_color(rgb):\n",
    "    r, g, b = rgb\n",
    "    if r > 240 and g > 240 and b > 240:\n",
    "        return True\n",
    "    if r < 15 and g < 15 and b < 15:\n",
    "        return True\n",
    "    max_diff = max(abs(r - g), abs(g - b), abs(r - b))\n",
    "    avg = (r + g + b) / 3\n",
    "    if max_diff < 20 and 100 < avg < 160:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def extract_colors(image_url, n_colors=3, timeout=10):\n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=timeout)\n",
    "        img = Image.open(BytesIO(response.content)).convert('RGB').resize((100, 100))\n",
    "        pixels = np.array(img).reshape(-1, 3)\n",
    "        \n",
    "        non_bg = np.array([p for p in pixels if not is_background_color(tuple(p))])\n",
    "        if len(non_bg) < 50:\n",
    "            non_bg = pixels\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=min(n_colors + 2, len(non_bg)), random_state=42, n_init=10)\n",
    "        kmeans.fit(non_bg)\n",
    "        \n",
    "        counts = Counter(kmeans.labels_)\n",
    "        total = len(kmeans.labels_)\n",
    "        \n",
    "        colors = []\n",
    "        for cluster_id, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            rgb = tuple(int(c) for c in kmeans.cluster_centers_[cluster_id])\n",
    "            if not is_background_color(rgb):\n",
    "                colors.append({\n",
    "                    'name': closest_standard_color(rgb),\n",
    "                    'weight': count / total\n",
    "                })\n",
    "            if len(colors) >= n_colors:\n",
    "                break\n",
    "        \n",
    "        return colors if colors else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Check for cache\n",
    "if os.path.exists(COLOR_CACHE_PATH):\n",
    "    print(f\"✓ Loading cached colors from: {COLOR_CACHE_PATH}\")\n",
    "    color_df = pd.read_csv(COLOR_CACHE_PATH, index_col=0)\n",
    "    print(f\"  Loaded {len(color_df)} color records\")\n",
    "else:\n",
    "    print(f\"⚠ No cache found. Extracting colors for filtered products...\")\n",
    "    print(f\"  This will take a while...\")\n",
    "    \n",
    "    # Prioritize explicitly gendered products for color extraction\n",
    "    gendered = df[df['label_extracted'].isin(['female', 'male'])].copy()\n",
    "    none_sample = df[df['label_extracted'] == 'none'].sample(\n",
    "        n=min(500, len(df[df['label_extracted'] == 'none'])), \n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Combine: all gendered + sample of none\n",
    "    to_extract = pd.concat([gendered, none_sample]).drop_duplicates()\n",
    "    to_extract = to_extract[to_extract[COL_IMAGE].notna()]\n",
    "    \n",
    "    if len(to_extract) > COLOR_EXTRACTION_SAMPLE:\n",
    "        to_extract = to_extract.sample(n=COLOR_EXTRACTION_SAMPLE, random_state=RANDOM_STATE)\n",
    "    \n",
    "    print(f\"  Extracting colors for {len(to_extract)} products...\")\n",
    "    \n",
    "    color_results = []\n",
    "    failed = 0\n",
    "    \n",
    "    for idx, (row_idx, row) in enumerate(to_extract.iterrows()):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"    Progress: {idx}/{len(to_extract)} ({100*idx/len(to_extract):.1f}%)\")\n",
    "        \n",
    "        colors = extract_colors(row[COL_IMAGE], n_colors=N_COLORS)\n",
    "        \n",
    "        if colors:\n",
    "            entry = {\n",
    "                'original_index': row_idx,\n",
    "                COL_PRODUCT_ID: row[COL_PRODUCT_ID],\n",
    "                'label_extracted': row['label_extracted']\n",
    "            }\n",
    "            for i, c in enumerate(colors):\n",
    "                entry[f'color{i+1}_name'] = c['name']\n",
    "                entry[f'color{i+1}_weight'] = c['weight']\n",
    "            color_results.append(entry)\n",
    "        else:\n",
    "            failed += 1\n",
    "        \n",
    "        time.sleep(0.05)\n",
    "    \n",
    "    color_df = pd.DataFrame(color_results)\n",
    "    if len(color_df) > 0:\n",
    "        color_df = color_df.set_index('original_index')\n",
    "    \n",
    "    # Save cache\n",
    "    color_df.to_csv(COLOR_CACHE_PATH)\n",
    "    print(f\"\\n✓ Color extraction complete. Saved to: {COLOR_CACHE_PATH}\")\n",
    "    print(f\"  Success: {len(color_df)}, Failed: {failed}\")\n",
    "\n",
    "print(f\"\\nColor data summary:\")\n",
    "if 'label_extracted' in color_df.columns:\n",
    "    print(color_df['label_extracted'].value_counts().to_dict())\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: PREPARE TRAINING DATA (NO COLOR REQUIREMENT)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: PREPARE TRAINING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get ALL explicitly gendered products (no color requirement!)\n",
    "female_all = df[df['label_extracted'] == 'female'].copy()\n",
    "male_all = df[df['label_extracted'] == 'male'].copy()\n",
    "\n",
    "print(f\"Explicitly female: {len(female_all)}\")\n",
    "print(f\"Explicitly male: {len(male_all)}\")\n",
    "\n",
    "# For \"none\" class: human-coded none + sample from extracted none\n",
    "human_none = df[(df['label_human'] == 'none')].copy()\n",
    "extracted_none = df[(df['label_extracted'] == 'none') & (df['label_human'].isna())].copy()\n",
    "\n",
    "# Target: balance to smallest gendered class\n",
    "min_gendered = min(len(female_all), len(male_all))\n",
    "target_none = min_gendered\n",
    "\n",
    "print(f\"\\nNone class sources:\")\n",
    "print(f\"  Human-coded none: {len(human_none)}\")\n",
    "print(f\"  Extracted none (unlabeled): {len(extracted_none)}\")\n",
    "\n",
    "# Build none class\n",
    "if len(human_none) >= target_none:\n",
    "    none_all = human_none.sample(n=target_none, random_state=RANDOM_STATE)\n",
    "else:\n",
    "    remaining = target_none - len(human_none)\n",
    "    sampled_none = extracted_none.sample(n=min(remaining, len(extracted_none)), random_state=RANDOM_STATE)\n",
    "    none_all = pd.concat([human_none, sampled_none])\n",
    "\n",
    "print(f\"  Final none class: {len(none_all)}\")\n",
    "\n",
    "# Balance all classes\n",
    "min_class = min(len(female_all), len(male_all), len(none_all))\n",
    "print(f\"\\nBalancing to: {min_class} per class\")\n",
    "\n",
    "female_balanced = female_all.sample(n=min_class, random_state=RANDOM_STATE)\n",
    "male_balanced = male_all.sample(n=min_class, random_state=RANDOM_STATE)\n",
    "none_balanced = none_all.sample(n=min(min_class, len(none_all)), random_state=RANDOM_STATE)\n",
    "\n",
    "# Combine\n",
    "ml_data = pd.concat([female_balanced, male_balanced, none_balanced]).copy()\n",
    "\n",
    "# Create target\n",
    "ml_data['target'] = ml_data['label_extracted'].map({'female': 0, 'male': 1})\n",
    "ml_data.loc[ml_data['target'].isna(), 'target'] = 2  # none\n",
    "ml_data['target'] = ml_data['target'].astype(int)\n",
    "\n",
    "print(f\"\\n✓ Training data: {len(ml_data)} products\")\n",
    "print(f\"  Class distribution: {ml_data['target'].value_counts().sort_index().to_dict()}\")\n",
    "print(f\"  (0=female, 1=male, 2=none)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: TRAIN-TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    ml_data.index,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=ml_data['target']\n",
    ")\n",
    "\n",
    "train_data = ml_data.loc[train_idx].copy()\n",
    "test_data = ml_data.loc[test_idx].copy()\n",
    "\n",
    "print(f\"Train: {len(train_data)}\")\n",
    "print(f\"Test: {len(test_data)}\")\n",
    "print(f\"Train distribution: {train_data['target'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def clean_text_remove_gender(text, remove_words=ALL_GENDER_KEYWORDS):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = str(text).lower()\n",
    "    for word in remove_words:\n",
    "        text = re.sub(r'\\b' + word + r'\\b', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Clean text\n",
    "for dataset in [train_data, test_data, df]:\n",
    "    dataset['breadcrumb_clean'] = dataset[COL_BREADCRUMB].apply(clean_text_remove_gender)\n",
    "    dataset['description_clean'] = dataset[COL_DESC].apply(clean_text_remove_gender)\n",
    "\n",
    "# --- Price features ---\n",
    "price_features = ['feat_price_log', 'feat_unit_price']\n",
    "for dataset in [train_data, test_data, df]:\n",
    "    dataset['feat_price'] = pd.to_numeric(dataset[COL_PRICE], errors='coerce')\n",
    "    dataset['feat_price_log'] = np.log1p(dataset['feat_price'])\n",
    "    if COL_UNIT_PRICE in dataset.columns:\n",
    "        dataset['feat_unit_price'] = dataset[COL_UNIT_PRICE].astype(str).str.extract(r'([\\d.]+)')[0].astype(float)\n",
    "    else:\n",
    "        dataset['feat_unit_price'] = 0\n",
    "print(f\"✓ Price features: {len(price_features)}\")\n",
    "\n",
    "# --- Store features ---\n",
    "store_encoder = LabelEncoder()\n",
    "store_encoder.fit(train_data[COL_STORE].fillna('unknown'))\n",
    "\n",
    "def encode_stores(data, encoder):\n",
    "    stores = data[COL_STORE].fillna('unknown')\n",
    "    encoded = []\n",
    "    for s in stores:\n",
    "        if s in encoder.classes_:\n",
    "            encoded.append(encoder.transform([s])[0])\n",
    "        else:\n",
    "            encoded.append(-1)\n",
    "    return np.array(encoded)\n",
    "\n",
    "train_data['store_encoded'] = encode_stores(train_data, store_encoder)\n",
    "test_data['store_encoded'] = encode_stores(test_data, store_encoder)\n",
    "df['store_encoded'] = encode_stores(df, store_encoder)\n",
    "\n",
    "n_stores = len(store_encoder.classes_) + 1\n",
    "print(f\"✓ Store features: {n_stores}\")\n",
    "\n",
    "# --- Breadcrumb TF-IDF (fit on TRAIN only) ---\n",
    "breadcrumb_vectorizer = TfidfVectorizer(\n",
    "    max_features=200,\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english'\n",
    ")\n",
    "breadcrumb_vectorizer.fit(train_data['breadcrumb_clean'])\n",
    "print(f\"✓ Breadcrumb TF-IDF: {len(breadcrumb_vectorizer.get_feature_names_out())} features\")\n",
    "\n",
    "# --- Description TF-IDF (fit on TRAIN only) ---\n",
    "description_vectorizer = TfidfVectorizer(\n",
    "    max_features=500,\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english'\n",
    ")\n",
    "description_vectorizer.fit(train_data['description_clean'])\n",
    "print(f\"✓ Description TF-IDF: {len(description_vectorizer.get_feature_names_out())} features\")\n",
    "\n",
    "# --- Color features (OPTIONAL - not all products have them) ---\n",
    "color_feature_cols = []\n",
    "for color_name in STANDARD_COLORS.keys():\n",
    "    for i in range(1, N_COLORS + 1):\n",
    "        col_name = f'feat_color{i}_{color_name}'\n",
    "        color_feature_cols.append(col_name)\n",
    "        for dataset in [train_data, test_data, df]:\n",
    "            dataset[col_name] = 0.0\n",
    "\n",
    "# Fill color features where available\n",
    "for dataset in [train_data, test_data, df]:\n",
    "    for idx in dataset.index:\n",
    "        if idx in color_df.index:\n",
    "            for i in range(1, N_COLORS + 1):\n",
    "                cname = color_df.loc[idx, f'color{i}_name'] if f'color{i}_name' in color_df.columns else None\n",
    "                cweight = color_df.loc[idx, f'color{i}_weight'] if f'color{i}_weight' in color_df.columns else 0\n",
    "                if pd.notna(cname) and cname in STANDARD_COLORS:\n",
    "                    dataset.loc[idx, f'feat_color{i}_{cname}'] = cweight\n",
    "\n",
    "# Count how many training samples have color data\n",
    "train_has_color = train_data.index.isin(color_df.index).sum()\n",
    "print(f\"✓ Color features: {len(color_feature_cols)} (available for {train_has_color}/{len(train_data)} train samples)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: BUILD FEATURE MATRICES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 7: BUILD FEATURE MATRICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def build_feature_matrix(data, bc_vec, desc_vec, color_cols, price_cols, n_stores, include_colors=True):\n",
    "    \"\"\"Build feature matrix\"\"\"\n",
    "    feature_names = []\n",
    "    blocks = []\n",
    "    \n",
    "    # Price\n",
    "    X_price = data[price_cols].fillna(0).values\n",
    "    blocks.append(csr_matrix(X_price))\n",
    "    feature_names.extend(price_cols)\n",
    "    \n",
    "    # Store (one-hot)\n",
    "    store_enc = data['store_encoded'].values\n",
    "    X_store = np.zeros((len(data), n_stores))\n",
    "    for i, s in enumerate(store_enc):\n",
    "        if s >= 0:\n",
    "            X_store[i, s] = 1\n",
    "        else:\n",
    "            X_store[i, -1] = 1\n",
    "    blocks.append(csr_matrix(X_store))\n",
    "    feature_names.extend([f'store_{i}' for i in range(n_stores)])\n",
    "    \n",
    "    # Breadcrumb TF-IDF\n",
    "    X_bc = bc_vec.transform(data['breadcrumb_clean'])\n",
    "    blocks.append(X_bc)\n",
    "    feature_names.extend([f'bc_{f}' for f in bc_vec.get_feature_names_out()])\n",
    "    \n",
    "    # Description TF-IDF\n",
    "    X_desc = desc_vec.transform(data['description_clean'])\n",
    "    blocks.append(X_desc)\n",
    "    feature_names.extend([f'desc_{f}' for f in desc_vec.get_feature_names_out()])\n",
    "    \n",
    "    # Color (optional)\n",
    "    if include_colors:\n",
    "        X_color = data[color_cols].values\n",
    "        blocks.append(csr_matrix(X_color))\n",
    "        feature_names.extend(color_cols)\n",
    "    \n",
    "    X = hstack(blocks)\n",
    "    return X, feature_names\n",
    "\n",
    "# Build with colors\n",
    "X_train, feature_names = build_feature_matrix(\n",
    "    train_data, breadcrumb_vectorizer, description_vectorizer,\n",
    "    color_feature_cols, price_features, n_stores, include_colors=True\n",
    ")\n",
    "X_test, _ = build_feature_matrix(\n",
    "    test_data, breadcrumb_vectorizer, description_vectorizer,\n",
    "    color_feature_cols, price_features, n_stores, include_colors=True\n",
    ")\n",
    "\n",
    "y_train = train_data['target'].values\n",
    "y_test = test_data['target'].values\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "\n",
    "# Validate\n",
    "assert X_train.shape[1] == len(feature_names), f\"Mismatch: {X_train.shape[1]} vs {len(feature_names)}\"\n",
    "print(f\"✓ Feature alignment validated\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: TRAIN MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 8: TRAIN MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "\n",
    "# --- L1 Logistic Regression ---\n",
    "print(\"\\n--- Logistic Regression (L1) ---\")\n",
    "model_l1 = LogisticRegressionCV(\n",
    "    cv=CV_FOLDS,\n",
    "    penalty='l1',\n",
    "    solver='saga',\n",
    "    max_iter=2000,\n",
    "    multi_class='multinomial',\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "model_l1.fit(X_train, y_train)\n",
    "y_pred_l1 = model_l1.predict(X_test)\n",
    "acc_l1 = accuracy_score(y_test, y_pred_l1)\n",
    "f1_l1 = f1_score(y_test, y_pred_l1, average='weighted')\n",
    "print(f\"Accuracy: {acc_l1:.4f}, F1: {f1_l1:.4f}\")\n",
    "results.append({'Model': 'L1 (LASSO)', 'Accuracy': acc_l1, 'F1_weighted': f1_l1})\n",
    "\n",
    "# --- L2 Logistic Regression ---\n",
    "print(\"\\n--- Logistic Regression (L2) ---\")\n",
    "model_l2 = LogisticRegressionCV(\n",
    "    cv=CV_FOLDS,\n",
    "    penalty='l2',\n",
    "    solver='lbfgs',\n",
    "    max_iter=2000,\n",
    "    multi_class='multinomial',\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "model_l2.fit(X_train, y_train)\n",
    "y_pred_l2 = model_l2.predict(X_test)\n",
    "acc_l2 = accuracy_score(y_test, y_pred_l2)\n",
    "f1_l2 = f1_score(y_test, y_pred_l2, average='weighted')\n",
    "print(f\"Accuracy: {acc_l2:.4f}, F1: {f1_l2:.4f}\")\n",
    "results.append({'Model': 'L2 (Ridge)', 'Accuracy': acc_l2, 'F1_weighted': f1_l2})\n",
    "\n",
    "# --- Random Forest ---\n",
    "print(\"\\n--- Random Forest ---\")\n",
    "model_rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "print(f\"Accuracy: {acc_rf:.4f}, F1: {f1_rf:.4f}\")\n",
    "results.append({'Model': 'Random Forest', 'Accuracy': acc_rf, 'F1_weighted': f1_rf})\n",
    "\n",
    "# --- Histogram Gradient Boosting ---\n",
    "print(\"\\n--- Histogram Gradient Boosting ---\")\n",
    "model_hgb = HistGradientBoostingClassifier(\n",
    "    max_iter=200,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "model_hgb.fit(X_train.toarray(), y_train)\n",
    "y_pred_hgb = model_hgb.predict(X_test.toarray())\n",
    "acc_hgb = accuracy_score(y_test, y_pred_hgb)\n",
    "f1_hgb = f1_score(y_test, y_pred_hgb, average='weighted')\n",
    "print(f\"Accuracy: {acc_hgb:.4f}, F1: {f1_hgb:.4f}\")\n",
    "results.append({'Model': 'Hist Gradient Boosting', 'Accuracy': acc_hgb, 'F1_weighted': f1_hgb})\n",
    "\n",
    "# --- SVM ---\n",
    "print(\"\\n--- SVM (RBF) ---\")\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_svm = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    class_weight='balanced',\n",
    "    probability=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "model_svm.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = model_svm.predict(X_test_scaled)\n",
    "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
    "print(f\"Accuracy: {acc_svm:.4f}, F1: {f1_svm:.4f}\")\n",
    "results.append({'Model': 'SVM', 'Accuracy': acc_svm, 'F1_weighted': f1_svm})\n",
    "\n",
    "# Results\n",
    "results_df = pd.DataFrame(results).sort_values('F1_weighted', ascending=False)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "results_df.to_csv(f'{OUTPUT_DIR}/model_comparison.csv', index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: BEST MODEL ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST MODEL ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_name = results_df.iloc[0]['Model']\n",
    "print(f\"Best model: {best_name}\")\n",
    "\n",
    "# Get best predictions\n",
    "if 'L1' in best_name:\n",
    "    best_model = model_l1\n",
    "    y_pred_best = y_pred_l1\n",
    "elif 'L2' in best_name:\n",
    "    best_model = model_l2\n",
    "    y_pred_best = y_pred_l2\n",
    "elif 'Random' in best_name:\n",
    "    best_model = model_rf\n",
    "    y_pred_best = y_pred_rf\n",
    "elif 'Hist' in best_name:\n",
    "    best_model = model_hgb\n",
    "    y_pred_best = y_pred_hgb\n",
    "else:\n",
    "    best_model = model_svm\n",
    "    y_pred_best = y_pred_svm\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['female', 'male', 'none']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(f\"            Predicted\")\n",
    "print(f\"            female  male  none\")\n",
    "for i, label in enumerate(['female', 'male', 'none']):\n",
    "    row = cm[i] if i < len(cm) else [0, 0, 0]\n",
    "    print(f\"Actual {label:6s}  {row[0]:4d}  {row[1]:4d}  {row[2]:4d}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE (L1 Model)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coef_female': model_l1.coef_[0],\n",
    "    'coef_male': model_l1.coef_[1],\n",
    "    'coef_none': model_l1.coef_[2]\n",
    "})\n",
    "importance_df['max_abs'] = importance_df[['coef_female', 'coef_male', 'coef_none']].abs().max(axis=1)\n",
    "importance_df = importance_df.sort_values('max_abs', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 FEMALE features:\")\n",
    "for _, row in importance_df[importance_df['coef_female'] > 0].nlargest(15, 'coef_female').iterrows():\n",
    "    print(f\"  {row['feature']:40s}: {row['coef_female']:+.4f}\")\n",
    "\n",
    "print(\"\\nTop 15 MALE features:\")\n",
    "for _, row in importance_df[importance_df['coef_male'] > 0].nlargest(15, 'coef_male').iterrows():\n",
    "    print(f\"  {row['feature']:40s}: {row['coef_male']:+.4f}\")\n",
    "\n",
    "print(\"\\nTop 15 NONE features:\")\n",
    "for _, row in importance_df[importance_df['coef_none'] > 0].nlargest(15, 'coef_none').iterrows():\n",
    "    print(f\"  {row['feature']:40s}: {row['coef_none']:+.4f}\")\n",
    "\n",
    "importance_df.to_csv(f'{OUTPUT_DIR}/feature_importance.csv', index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 11: PREDICT ON ALL PRODUCTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 11: PREDICT ON ALL PRODUCTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_all, _ = build_feature_matrix(\n",
    "    df, breadcrumb_vectorizer, description_vectorizer,\n",
    "    color_feature_cols, price_features, n_stores, include_colors=True\n",
    ")\n",
    "\n",
    "print(f\"Full dataset: {X_all.shape}\")\n",
    "\n",
    "# Use L1 for interpretability\n",
    "df['ml_prob_female'] = model_l1.predict_proba(X_all)[:, 0]\n",
    "df['ml_prob_male'] = model_l1.predict_proba(X_all)[:, 1]\n",
    "df['ml_prob_none'] = model_l1.predict_proba(X_all)[:, 2]\n",
    "df['ml_pred'] = model_l1.predict(X_all)\n",
    "df['ml_pred_label'] = df['ml_pred'].map({0: 'female', 1: 'male', 2: 'none'})\n",
    "df['ml_confidence'] = df[['ml_prob_female', 'ml_prob_male', 'ml_prob_none']].max(axis=1)\n",
    "\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(df['ml_pred_label'].value_counts())\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 12: COMPARE WITH/WITHOUT COLOR FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 12: COLOR FEATURE IMPACT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train model WITHOUT colors\n",
    "X_train_no_color, features_no_color = build_feature_matrix(\n",
    "    train_data, breadcrumb_vectorizer, description_vectorizer,\n",
    "    color_feature_cols, price_features, n_stores, include_colors=False\n",
    ")\n",
    "X_test_no_color, _ = build_feature_matrix(\n",
    "    test_data, breadcrumb_vectorizer, description_vectorizer,\n",
    "    color_feature_cols, price_features, n_stores, include_colors=False\n",
    ")\n",
    "\n",
    "model_no_color = LogisticRegressionCV(\n",
    "    cv=CV_FOLDS, penalty='l1', solver='saga', max_iter=2000,\n",
    "    multi_class='multinomial', class_weight='balanced', random_state=RANDOM_STATE\n",
    ")\n",
    "model_no_color.fit(X_train_no_color, y_train)\n",
    "y_pred_no_color = model_no_color.predict(X_test_no_color)\n",
    "\n",
    "acc_no_color = accuracy_score(y_test, y_pred_no_color)\n",
    "f1_no_color = f1_score(y_test, y_pred_no_color, average='weighted')\n",
    "\n",
    "print(f\"WITH colors:    Accuracy={acc_l1:.4f}, F1={f1_l1:.4f}\")\n",
    "print(f\"WITHOUT colors: Accuracy={acc_no_color:.4f}, F1={f1_no_color:.4f}\")\n",
    "print(f\"Color impact:   Accuracy {'+' if acc_l1 > acc_no_color else ''}{(acc_l1-acc_no_color)*100:.2f}pp, F1 {'+' if f1_l1 > f1_no_color else ''}{(f1_l1-f1_no_color)*100:.2f}pp\")\n",
    "\n",
    "# On subset WITH color data only\n",
    "train_with_color = train_data[train_data.index.isin(color_df.index)]\n",
    "test_with_color = test_data[test_data.index.isin(color_df.index)]\n",
    "\n",
    "if len(test_with_color) > 10:\n",
    "    print(f\"\\nOn color-extracted subset only ({len(test_with_color)} test samples):\")\n",
    "    \n",
    "    X_test_color_subset, _ = build_feature_matrix(\n",
    "        test_with_color, breadcrumb_vectorizer, description_vectorizer,\n",
    "        color_feature_cols, price_features, n_stores, include_colors=True\n",
    "    )\n",
    "    X_test_color_subset_no_color, _ = build_feature_matrix(\n",
    "        test_with_color, breadcrumb_vectorizer, description_vectorizer,\n",
    "        color_feature_cols, price_features, n_stores, include_colors=False\n",
    "    )\n",
    "    \n",
    "    y_test_subset = test_with_color['target'].values\n",
    "    \n",
    "    pred_with = model_l1.predict(X_test_color_subset)\n",
    "    pred_without = model_no_color.predict(X_test_color_subset_no_color)\n",
    "    \n",
    "    acc_with = accuracy_score(y_test_subset, pred_with)\n",
    "    acc_without = accuracy_score(y_test_subset, pred_without)\n",
    "    \n",
    "    print(f\"  WITH colors:    Accuracy={acc_with:.4f}\")\n",
    "    print(f\"  WITHOUT colors: Accuracy={acc_without:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 13: VALIDATION VS HUMAN LABELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 13: VALIDATION VS HUMAN LABELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "human_labeled = df[df['label_human'].notna()].copy()\n",
    "print(f\"Products with human labels: {len(human_labeled)}\")\n",
    "\n",
    "if len(human_labeled) > 0:\n",
    "    human_labeled['human_encoded'] = human_labeled['label_human'].map({\n",
    "        'female': 0, 'male': 1, 'none': 2\n",
    "    })\n",
    "    valid = human_labeled[human_labeled['human_encoded'].notna()]\n",
    "    \n",
    "    if len(valid) > 0:\n",
    "        acc_human = accuracy_score(valid['human_encoded'], valid['ml_pred'])\n",
    "        print(f\"Accuracy vs human: {acc_human:.4f}\")\n",
    "        \n",
    "        print(\"\\nConfusion (ML vs Human):\")\n",
    "        cm_h = confusion_matrix(valid['human_encoded'], valid['ml_pred'], labels=[0, 1, 2])\n",
    "        print(f\"            ML Predicted\")\n",
    "        print(f\"            female  male  none\")\n",
    "        for i, label in enumerate(['female', 'male', 'none']):\n",
    "            print(f\"Human {label:6s}  {cm_h[i,0]:4d}  {cm_h[i,1]:4d}  {cm_h[i,2]:4d}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 14: IMPLICIT GENDERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 14: IMPLICIT GENDERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "implicit_female = df[\n",
    "    (df['label_extracted'] == 'none') &\n",
    "    (df['ml_pred_label'] == 'female') &\n",
    "    (df['ml_confidence'] > 0.5)\n",
    "]\n",
    "implicit_male = df[\n",
    "    (df['label_extracted'] == 'none') &\n",
    "    (df['ml_pred_label'] == 'male') &\n",
    "    (df['ml_confidence'] > 0.5)\n",
    "]\n",
    "predicted_none = df[df['ml_pred_label'] == 'none']\n",
    "\n",
    "print(f\"Implicit female (high conf): {len(implicit_female):,}\")\n",
    "print(f\"Implicit male (high conf): {len(implicit_male):,}\")\n",
    "print(f\"Predicted none: {len(predicted_none):,}\")\n",
    "\n",
    "print(f\"\\nSample implicit FEMALE:\")\n",
    "for _, row in implicit_female.head(5).iterrows():\n",
    "    print(f\"  {row[COL_NAME][:60]}... (conf: {row['ml_confidence']:.2f})\")\n",
    "\n",
    "print(f\"\\nSample implicit MALE:\")\n",
    "for _, row in implicit_male.head(5).iterrows():\n",
    "    print(f\"  {row[COL_NAME][:60]}... (conf: {row['ml_confidence']:.2f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 15: EXPORT VALIDATION SAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 15: EXPORT VALIDATION SAMPLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "already_labeled = set()\n",
    "if COL_PRODUCT_ID in human_coded.columns:\n",
    "    already_labeled.update(human_coded[COL_PRODUCT_ID].values)\n",
    "if len(your_labeled) > 0 and COL_PRODUCT_ID in your_labeled.columns:\n",
    "    already_labeled.update(your_labeled[COL_PRODUCT_ID].values)\n",
    "\n",
    "available = df[\n",
    "    (~df[COL_PRODUCT_ID].isin(already_labeled)) &\n",
    "    (df[COL_IMAGE].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"Available: {len(available):,}\")\n",
    "\n",
    "# Sample balanced\n",
    "N_PER = 85\n",
    "samples = []\n",
    "for pred, label in [(0, 'female'), (1, 'male'), (2, 'none')]:\n",
    "    pool = available[available['ml_pred'] == pred]\n",
    "    n = min(N_PER, len(pool))\n",
    "    if n > 0:\n",
    "        samples.append(pool.sample(n=n, random_state=RANDOM_STATE))\n",
    "        print(f\"  Sampled {n} {label}\")\n",
    "\n",
    "validation = pd.concat(samples).sample(frac=1, random_state=RANDOM_STATE)\n",
    "\n",
    "export_cols = [\n",
    "    COL_PRODUCT_ID, COL_NAME, COL_DESC, COL_BREADCRUMB, COL_IMAGE, COL_URL, COL_PRICE,\n",
    "    'label_extracted', 'ml_pred_label', 'ml_prob_female', 'ml_prob_male', 'ml_prob_none', 'ml_confidence'\n",
    "]\n",
    "export_cols = [c for c in export_cols if c in validation.columns]\n",
    "\n",
    "validation_export = validation[export_cols].copy()\n",
    "validation_export['manual_gender'] = ''\n",
    "validation_export['manual_confidence'] = ''\n",
    "validation_export['manual_notes'] = ''\n",
    "\n",
    "output_file = f'{OUTPUT_DIR}/validation_sample.csv'\n",
    "validation_export.to_csv(output_file, index=True)\n",
    "print(f\"\\n✓ Saved: {output_file} ({len(validation_export)} products)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE SUMMARY (v3)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATA:\n",
    "  Original: {original_count:,}\n",
    "  After filtering: {len(df):,}\n",
    "  Excluded categories: {excluded_count:,}\n",
    "\n",
    "TRAINING:\n",
    "  Total samples: {len(ml_data):,} (balanced 3-class)\n",
    "  Train: {len(train_data):,}\n",
    "  Test: {len(test_data):,}\n",
    "  Products with colors: {len(color_df):,}\n",
    "\n",
    "BEST MODEL: {best_name} (F1: {results_df.iloc[0]['F1_weighted']:.4f})\n",
    "\n",
    "COLOR IMPACT:\n",
    "  With colors:    F1={f1_l1:.4f}\n",
    "  Without colors: F1={f1_no_color:.4f}\n",
    "\n",
    "PREDICTIONS:\n",
    "  Female: {(df['ml_pred_label'] == 'female').sum():,}\n",
    "  Male: {(df['ml_pred_label'] == 'male').sum():,}\n",
    "  None: {(df['ml_pred_label'] == 'none').sum():,}\n",
    "\n",
    "IMPLICIT GENDERING:\n",
    "  Implicit female: {len(implicit_female):,}\n",
    "  Implicit male: {len(implicit_male):,}\n",
    "\n",
    "OUTPUT:\n",
    "  {OUTPUT_DIR}/\n",
    "  ├── model_comparison.csv\n",
    "  ├── feature_importance.csv\n",
    "  └── validation_sample.csv\n",
    "\"\"\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'version': '3.0',\n",
    "    'data': {\n",
    "        'original': original_count,\n",
    "        'filtered': len(df),\n",
    "        'excluded': excluded_count,\n",
    "        'training_samples': len(ml_data),\n",
    "        'color_samples': len(color_df)\n",
    "    },\n",
    "    'models': results,\n",
    "    'color_impact': {\n",
    "        'with_colors_f1': f1_l1,\n",
    "        'without_colors_f1': f1_no_color\n",
    "    },\n",
    "    'predictions': {\n",
    "        'female': int((df['ml_pred_label'] == 'female').sum()),\n",
    "        'male': int((df['ml_pred_label'] == 'male').sum()),\n",
    "        'none': int((df['ml_pred_label'] == 'none').sum())\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Pipeline complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
