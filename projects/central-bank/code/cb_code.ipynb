{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55e4d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '–' (U+2013) (3491157562.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    <title>Reinforcement Learning for Monetary Policy – Leonardo Luksic</title>\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '–' (U+2013)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reinforcement Learning for Monetary Policy Optimization\n",
    "========================================================\n",
    "A DQN agent trained on monthly US macroeconomic data (1971–2025) to set\n",
    "interest rates, benchmarked against the Taylor Rule and actual Fed decisions.\n",
    "\n",
    "Authors: Leonardo Luksic, Krisha Chandnani, Ignacio Orueta\n",
    "LSE — February 2026\n",
    "\n",
    "Pipeline overview:\n",
    "  1. Load monthly FRED data (CPI, unemployment, capacity utilisation,\n",
    "     fed funds rate, 10-year Treasury, NFCI)\n",
    "  2. Engineer features with realistic central-bank information lags\n",
    "     (18/24/30 months) and a 12-month-ahead inflation target\n",
    "  3. Train neural-network inflation forecasters under three specifications,\n",
    "     validated with expanding-window time-series cross-validation\n",
    "  4. Build a Gymnasium environment where the agent's rate choice feeds\n",
    "     back into the transition dynamics\n",
    "  5. Train a DQN agent (PyTorch) and compare its policy against the\n",
    "     standard Taylor Rule and actual Federal Reserve decisions\n",
    "  6. Generate portfolio-ready visualisations\n",
    "\n",
    "Key methodological choices:\n",
    "  - Features use only information realistically available to a central bank\n",
    "    (accounting for publication lags of ~6 months plus a 12-month lookback)\n",
    "  - The forecast horizon is 12 months ahead, matching actual policy horizons\n",
    "  - Time-series CV avoids data leakage across economic regimes\n",
    "  - The environment propagates the agent's rate choice into future features\n",
    "    via a rolling lag buffer, so actions have real consequences\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "DEFAULT_CFG = {\n",
    "    \"data_path\": \"./data\",\n",
    "    \"output_path\": \"./outputs\",\n",
    "\n",
    "    # Targets\n",
    "    \"inflation_target\": 2.0,\n",
    "    \"unemployment_natural\": 5.0,\n",
    "\n",
    "    # Environment\n",
    "    \"max_steps\": 36,          # 3-year episodes\n",
    "    \"n_actions\": 41,          # 0–20 % in 0.5 pp increments\n",
    "    \"min_rate\": 0.0,\n",
    "    \"max_rate\": 20.0,\n",
    "    \"omega_pi\": 1.0,          # inflation-deviation weight\n",
    "    \"omega_u\": 0.5,           # unemployment-deviation weight\n",
    "    \"omega_smooth\": 0.1,      # rate-smoothing weight\n",
    "\n",
    "    # DQN agent\n",
    "    \"buffer_capacity\": 15_000,\n",
    "    \"batch_size\": 64,\n",
    "    \"gamma\": 0.99,\n",
    "    \"epsilon_start\": 1.0,\n",
    "    \"epsilon_end\": 0.02,\n",
    "    \"epsilon_decay_steps\": 8_000,\n",
    "    \"lr\": 3e-4,\n",
    "    \"target_update_freq\": 400,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"train_start_step\": 500,\n",
    "    \"n_episodes\": 600,\n",
    "\n",
    "    # Cross-validation folds (expanding window)\n",
    "    \"cv_folds\": [\n",
    "        {\"train_end\": \"1990-12\", \"test_start\": \"1991-01\",\n",
    "         \"test_end\": \"1995-12\", \"name\": \"Early 1990s\"},\n",
    "        {\"train_end\": \"1995-12\", \"test_start\": \"1996-01\",\n",
    "         \"test_end\": \"2000-12\", \"name\": \"Late 1990s\"},\n",
    "        {\"train_end\": \"2000-12\", \"test_start\": \"2001-01\",\n",
    "         \"test_end\": \"2007-12\", \"name\": \"2000s\"},\n",
    "        {\"train_end\": \"2007-12\", \"test_start\": \"2008-01\",\n",
    "         \"test_end\": \"2015-12\", \"name\": \"GFC & aftermath\"},\n",
    "        {\"train_end\": \"2015-12\", \"test_start\": \"2016-01\",\n",
    "         \"test_end\": \"2024-12\", \"name\": \"Recent\"},\n",
    "    ],\n",
    "\n",
    "    \"fig_dpi\": 200,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Data loading\n",
    "# ============================================================================\n",
    "\n",
    "def _try_read(data_path, options):\n",
    "    \"\"\"Try several filenames; return the first that exists.\"\"\"\n",
    "    for name in options:\n",
    "        p = os.path.join(data_path, name)\n",
    "        if os.path.exists(p):\n",
    "            return pd.read_csv(p, parse_dates=[\"observation_date\"],\n",
    "                               index_col=\"observation_date\")\n",
    "    raise FileNotFoundError(f\"None of {options} found in {data_path}\")\n",
    "\n",
    "\n",
    "def load_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load monthly FRED CSVs and merge into a single panel.\n",
    "\n",
    "    Variables:\n",
    "        CPI, unemployment, fed funds rate, capacity utilisation,\n",
    "        10-year Treasury yield, National Financial Conditions Index\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"LOADING MONTHLY FRED DATA\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    cpi = _try_read(data_path, [\"CPIAUCSL.csv\"])\n",
    "    unrate = _try_read(data_path, [\"UNRATE.csv\"])\n",
    "    ff = _try_read(data_path, [\"FEDFUNDS.csv\", \"FEDFUNDS-1.csv\"])\n",
    "    tcu = _try_read(data_path, [\"TCU.csv\"])\n",
    "    gs10 = _try_read(data_path, [\"GS10.csv\"])\n",
    "    nfci_raw = _try_read(data_path, [\"NFCI.csv\"])\n",
    "\n",
    "    # NFCI is weekly — resample to month-start\n",
    "    nfci = nfci_raw.resample(\"MS\").last()\n",
    "\n",
    "    merged = pd.concat([\n",
    "        cpi.rename(columns={cpi.columns[0]: \"cpi\"}),\n",
    "        unrate.rename(columns={unrate.columns[0]: \"unemployment\"}),\n",
    "        ff.rename(columns={ff.columns[0]: \"fed_funds\"}),\n",
    "        tcu.rename(columns={tcu.columns[0]: \"capacity_util\"}),\n",
    "        gs10.rename(columns={gs10.columns[0]: \"treasury_10y\"}),\n",
    "        nfci.rename(columns={nfci.columns[0]: \"fin_conditions\"}),\n",
    "    ], axis=1).dropna()\n",
    "\n",
    "    print(f\"  Merged panel: {len(merged)} monthly obs  \"\n",
    "          f\"({merged.index.min():%Y-%m} to {merged.index.max():%Y-%m})\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Feature engineering\n",
    "# ============================================================================\n",
    "\n",
    "def engineer_features(data: pd.DataFrame,\n",
    "                      natural_rate: float = 5.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create derived indicators and lagged features at two horizons:\n",
    "\n",
    "    - Realistic lags (18, 24, 30 months): approximate the information\n",
    "      set available to a central bank after publication delays.\n",
    "    - Intermediate lags (3, 6, 12 months): provide more recent signal\n",
    "      that improves forecast quality.  Less realistic about data\n",
    "      availability, but useful for comparing model performance.\n",
    "\n",
    "    The target variable is inflation 12 months ahead.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    # Derived indicators\n",
    "    df[\"inflation\"] = df[\"cpi\"].pct_change(12) * 100\n",
    "    df[\"unemp_gap\"] = df[\"unemployment\"] - natural_rate\n",
    "    df[\"capacity_gap\"] = df[\"capacity_util\"] - 100.0\n",
    "    df[\"term_spread\"] = df[\"treasury_10y\"] - df[\"fed_funds\"]\n",
    "\n",
    "    # Lagged features — both intermediate and realistic horizons\n",
    "    lag_vars = [\"inflation\", \"unemp_gap\", \"capacity_gap\",\n",
    "                \"fed_funds\", \"term_spread\", \"fin_conditions\"]\n",
    "    all_lags = [3, 6, 12, 18, 24, 30]\n",
    "    for var in lag_vars:\n",
    "        for lag in all_lags:\n",
    "            df[f\"L{lag}_{var}\"] = df[var].shift(lag)\n",
    "\n",
    "    # Forward target: inflation 12 months from now\n",
    "    df[\"inflation_12m_ahead\"] = df[\"inflation\"].shift(-12)\n",
    "\n",
    "    df = df.dropna()\n",
    "    print(f\"  Final dataset: {len(df)} obs  \"\n",
    "          f\"({df.index.min():%Y-%m} to {df.index.max():%Y-%m})\")\n",
    "    print(f\"  Lags: {all_lags}  |  Target: inflation at t+12\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Model specifications\n",
    "# ============================================================================\n",
    "\n",
    "SPECS = {\n",
    "    \"SIMPLE\": {\n",
    "        \"features\": [\n",
    "            \"L18_inflation\", \"L24_inflation\", \"L30_inflation\",\n",
    "            \"L18_fed_funds\", \"L24_fed_funds\", \"L30_fed_funds\",\n",
    "        ],\n",
    "        \"desc\": \"Core variables, realistic lags only\",\n",
    "    },\n",
    "    \"EXPANDED\": {\n",
    "        \"features\": [\n",
    "            \"L18_inflation\", \"L24_inflation\", \"L30_inflation\",\n",
    "            \"L18_unemp_gap\", \"L24_unemp_gap\", \"L30_unemp_gap\",\n",
    "            \"L18_capacity_gap\", \"L24_capacity_gap\", \"L30_capacity_gap\",\n",
    "            \"L18_fed_funds\", \"L24_fed_funds\", \"L30_fed_funds\",\n",
    "        ],\n",
    "        \"desc\": \"With real-economy measures, realistic lags only\",\n",
    "    },\n",
    "    \"FULL\": {\n",
    "        \"features\": [\n",
    "            \"L18_inflation\", \"L24_inflation\", \"L30_inflation\",\n",
    "            \"L18_unemp_gap\", \"L24_unemp_gap\", \"L30_unemp_gap\",\n",
    "            \"L18_capacity_gap\", \"L24_capacity_gap\", \"L30_capacity_gap\",\n",
    "            \"L18_fed_funds\", \"L24_fed_funds\", \"L30_fed_funds\",\n",
    "            \"L18_term_spread\", \"L24_term_spread\", \"L30_term_spread\",\n",
    "            \"L18_fin_conditions\", \"L24_fin_conditions\", \"L30_fin_conditions\",\n",
    "        ],\n",
    "        \"desc\": \"Full specification, realistic lags only\",\n",
    "    },\n",
    "    \"INFORMATIVE\": {\n",
    "        \"features\": [\n",
    "            # Intermediate lags (more recent signal, less realistic)\n",
    "            \"L3_inflation\", \"L6_inflation\", \"L12_inflation\",\n",
    "            \"L3_unemp_gap\", \"L6_unemp_gap\", \"L12_unemp_gap\",\n",
    "            \"L3_capacity_gap\", \"L6_capacity_gap\", \"L12_capacity_gap\",\n",
    "            \"L3_fed_funds\", \"L6_fed_funds\", \"L12_fed_funds\",\n",
    "            \"L3_term_spread\", \"L6_term_spread\", \"L12_term_spread\",\n",
    "            \"L3_fin_conditions\", \"L6_fin_conditions\", \"L12_fin_conditions\",\n",
    "            # Realistic lags (longer-term context)\n",
    "            \"L18_inflation\", \"L24_inflation\", \"L30_inflation\",\n",
    "            \"L18_unemp_gap\", \"L24_unemp_gap\", \"L30_unemp_gap\",\n",
    "            \"L18_capacity_gap\", \"L24_capacity_gap\", \"L30_capacity_gap\",\n",
    "            \"L18_fed_funds\", \"L24_fed_funds\", \"L30_fed_funds\",\n",
    "            \"L18_term_spread\", \"L24_term_spread\", \"L30_term_spread\",\n",
    "            \"L18_fin_conditions\", \"L24_fin_conditions\", \"L30_fin_conditions\",\n",
    "        ],\n",
    "        \"desc\": \"All variables, intermediate + realistic lags (L3–L30)\",\n",
    "    },\n",
    "}\n",
    "\n",
    "TARGET_COL = \"inflation_12m_ahead\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Time-series cross-validation\n",
    "# ============================================================================\n",
    "\n",
    "def time_series_cv(historical: pd.DataFrame, folds: list) -> dict:\n",
    "    \"\"\"\n",
    "    Expanding-window CV across economic regimes.\n",
    "\n",
    "    Each fold trains on all data up to `train_end` and tests on the\n",
    "    subsequent window.  This avoids look-ahead bias and checks whether\n",
    "    the model generalises across structurally different periods\n",
    "    (e.g. Volcker disinflation, Great Moderation, GFC, post-COVID).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TIME-SERIES CROSS-VALIDATION\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    cv_results = {}\n",
    "\n",
    "    for spec_name, spec in SPECS.items():\n",
    "        print(f\"\\n  --- {spec_name} ({len(spec['features'])} features) ---\")\n",
    "        fold_metrics = []\n",
    "\n",
    "        for fold in folds:\n",
    "            train = historical.loc[:fold[\"train_end\"]]\n",
    "            test = historical.loc[fold[\"test_start\"]:fold[\"test_end\"]]\n",
    "            if len(test) < 10:\n",
    "                continue\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_tr = scaler.fit_transform(train[spec[\"features\"]])\n",
    "            X_te = scaler.transform(test[spec[\"features\"]])\n",
    "            y_tr, y_te = train[TARGET_COL], test[TARGET_COL]\n",
    "\n",
    "            model = MLPRegressor(\n",
    "                hidden_layer_sizes=(128, 64, 32), activation=\"relu\",\n",
    "                solver=\"adam\", alpha=0.001, batch_size=32,\n",
    "                learning_rate=\"adaptive\", learning_rate_init=0.001,\n",
    "                max_iter=500, early_stopping=True,\n",
    "                validation_fraction=0.15, n_iter_no_change=20,\n",
    "                random_state=42, verbose=False,\n",
    "            )\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred = model.predict(X_te)\n",
    "\n",
    "            mse = mean_squared_error(y_te, y_pred)\n",
    "            mae = mean_absolute_error(y_te, y_pred)\n",
    "            r2 = r2_score(y_te, y_pred)\n",
    "            fold_metrics.append({\"fold\": fold[\"name\"], \"mse\": mse,\n",
    "                                 \"mae\": mae, \"r2\": r2})\n",
    "            print(f\"    {fold['name']:18s}  MSE={mse:.4f}  \"\n",
    "                  f\"MAE={mae:.4f}  R²={r2:.4f}\")\n",
    "\n",
    "        avg = {k: np.mean([f[k] for f in fold_metrics])\n",
    "               for k in (\"mse\", \"mae\", \"r2\")}\n",
    "        cv_results[spec_name] = {\"folds\": fold_metrics, **avg}\n",
    "        print(f\"    {'Average':18s}  MSE={avg['mse']:.4f}  \"\n",
    "              f\"MAE={avg['mae']:.4f}  R²={avg['r2']:.4f}\")\n",
    "\n",
    "    best = min(cv_results, key=lambda s: cv_results[s][\"mse\"])\n",
    "    print(f\"\\n  Best specification: {best}  \"\n",
    "          f\"(avg CV MSE = {cv_results[best]['mse']:.4f})\")\n",
    "    return cv_results, best\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Final model training\n",
    "# ============================================================================\n",
    "\n",
    "def train_final_model(historical, spec_name):\n",
    "    \"\"\"Train on 80 % of data, validate on 20 %, return model + scaler.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"TRAINING FINAL MODEL: {spec_name}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    feats = SPECS[spec_name][\"features\"]\n",
    "    X, y = historical[feats], historical[TARGET_COL]\n",
    "    split = int(len(X) * 0.8)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X.iloc[:split])\n",
    "    X_val = scaler.transform(X.iloc[split:])\n",
    "    y_tr, y_val = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(128, 64, 32), activation=\"relu\",\n",
    "        solver=\"adam\", alpha=0.001, batch_size=32,\n",
    "        learning_rate=\"adaptive\", learning_rate_init=0.001,\n",
    "        max_iter=500, early_stopping=True,\n",
    "        validation_fraction=0.15, n_iter_no_change=20,\n",
    "        random_state=42, verbose=False,\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    print(f\"  Validation MSE: {mse:.4f}  |  R²: {r2:.4f}\")\n",
    "    return model, scaler, feats\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. Gymnasium environment\n",
    "# ============================================================================\n",
    "\n",
    "class MonetaryPolicyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Monthly monetary-policy environment.\n",
    "\n",
    "    The agent observes (inflation, unemployment, capacity utilisation,\n",
    "    current rate) and chooses a discrete interest rate.  The chosen rate\n",
    "    is written into a rolling lag buffer so that it enters the inflation\n",
    "    forecast model after 18 months — matching the realistic information\n",
    "    structure.  Unemployment and capacity evolve from the historical\n",
    "    record (the agent does not control these directly).\n",
    "\n",
    "    Reward = -(w_pi * (pi - pi*)^2 + w_u * (u - u*)^2 + w_s * (di)^2)\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, model, scaler, feature_cols, historical_df, cfg):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.feature_cols = feature_cols\n",
    "        self.hist = historical_df.reset_index(drop=True)\n",
    "\n",
    "        self.pi_target = cfg[\"inflation_target\"]\n",
    "        self.u_target = cfg[\"unemployment_natural\"]\n",
    "        self.w_pi = cfg[\"omega_pi\"]\n",
    "        self.w_u = cfg[\"omega_u\"]\n",
    "        self.w_smooth = cfg[\"omega_smooth\"]\n",
    "        self.max_steps = cfg[\"max_steps\"]\n",
    "\n",
    "        self.n_actions = cfg[\"n_actions\"]\n",
    "        self.rate_grid = np.linspace(cfg[\"min_rate\"], cfg[\"max_rate\"],\n",
    "                                     self.n_actions)\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        self.observation_space = spaces.Box(-np.inf, np.inf, (4,),\n",
    "                                            dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            super().reset(seed=seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        min_start = 30\n",
    "        max_start = len(self.hist) - self.max_steps - 12 - 1\n",
    "        if max_start <= min_start:\n",
    "            self.start_idx = min_start\n",
    "        else:\n",
    "            self.start_idx = np.random.randint(min_start, max_start)\n",
    "\n",
    "        self.idx = self.start_idx\n",
    "        self.step_count = 0\n",
    "        self._done = False\n",
    "\n",
    "        row = self.hist.iloc[self.idx]\n",
    "        self.state = np.array([row[\"inflation\"], row[\"unemployment\"],\n",
    "                               row[\"capacity_util\"], row[\"fed_funds\"]],\n",
    "                              dtype=np.float32)\n",
    "        self.prev_rate = row[\"fed_funds\"]\n",
    "\n",
    "        # Rolling buffer of agent's rate choices (for lag propagation)\n",
    "        # Pre-fill with historical rates at the start\n",
    "        self._rate_buffer = deque(\n",
    "            [self.hist.iloc[max(0, self.idx - i)][\"fed_funds\"]\n",
    "             for i in range(31)],\n",
    "            maxlen=31,\n",
    "        )\n",
    "\n",
    "        self.episode_history = []\n",
    "        self.cumulative_reward = 0.0\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self._done:\n",
    "            raise RuntimeError(\"Episode finished — call reset()\")\n",
    "\n",
    "        rate = float(self.rate_grid[int(action)])\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Push the agent's rate into the rolling buffer\n",
    "        self._rate_buffer.appendleft(rate)\n",
    "\n",
    "        # Build feature vector using historical data for all variables\n",
    "        # EXCEPT fed_funds lags, which come from the agent's rate buffer\n",
    "        row = self.hist.iloc[self.idx]\n",
    "        feat_dict = {}\n",
    "        for col in self.feature_cols:\n",
    "            if col in row.index:\n",
    "                # Override fed_funds lags with agent's actual choices\n",
    "                if \"fed_funds\" in col:\n",
    "                    lag = int(col.split(\"_\")[0][1:])  # e.g. \"L18\" -> 18\n",
    "                    if lag < len(self._rate_buffer):\n",
    "                        feat_dict[col] = [self._rate_buffer[lag]]\n",
    "                    else:\n",
    "                        feat_dict[col] = [row[col]]\n",
    "                else:\n",
    "                    feat_dict[col] = [row[col]]\n",
    "            else:\n",
    "                feat_dict[col] = [0.0]\n",
    "\n",
    "        features_scaled = self.scaler.transform(pd.DataFrame(feat_dict))\n",
    "        next_pi = float(self.model.predict(features_scaled)[0])\n",
    "        next_pi = np.clip(next_pi, -2.0, 15.0)\n",
    "\n",
    "        # Unemployment & capacity from historical record\n",
    "        next_idx = min(self.idx + 1, len(self.hist) - 1)\n",
    "        next_u = self.hist.iloc[next_idx][\"unemployment\"]\n",
    "        next_cap = self.hist.iloc[next_idx][\"capacity_util\"]\n",
    "\n",
    "        # Reward\n",
    "        reward = -(self.w_pi * (next_pi - self.pi_target) ** 2\n",
    "                   + self.w_u * (next_u - self.u_target) ** 2\n",
    "                   + self.w_smooth * (rate - self.prev_rate) ** 2)\n",
    "\n",
    "        self.episode_history.append({\n",
    "            \"inflation\": next_pi, \"unemployment\": next_u,\n",
    "            \"capacity\": next_cap, \"rate\": rate, \"reward\": reward})\n",
    "        self.cumulative_reward += reward\n",
    "\n",
    "        self.state = np.array([next_pi, next_u, next_cap, rate],\n",
    "                              dtype=np.float32)\n",
    "        self.prev_rate = rate\n",
    "        self.idx = next_idx\n",
    "\n",
    "        self._done = self.step_count >= self.max_steps\n",
    "        return self.state, reward, self._done, False, {}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. DQN agent (PyTorch)\n",
    "# ============================================================================\n",
    "\n",
    "Transition = namedtuple(\"Transition\",\n",
    "                        (\"state\", \"action\", \"reward\", \"next_state\", \"done\"))\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buf = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buf.append(Transition(*args))\n",
    "\n",
    "    def sample(self, n):\n",
    "        return random.sample(self.buf, n)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buf)\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, cfg):\n",
    "        self.env = env\n",
    "        sd = env.observation_space.shape[0]\n",
    "        ad = env.action_space.n\n",
    "\n",
    "        self.buffer = ReplayBuffer(cfg[\"buffer_capacity\"])\n",
    "        self.batch_size = cfg[\"batch_size\"]\n",
    "        self.gamma = cfg[\"gamma\"]\n",
    "\n",
    "        self.epsilon = cfg[\"epsilon_start\"]\n",
    "        self.eps_min = cfg[\"epsilon_end\"]\n",
    "        self.eps_decay = (cfg[\"epsilon_end\"] / cfg[\"epsilon_start\"]\n",
    "                          ) ** (1.0 / cfg[\"epsilon_decay_steps\"])\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = QNetwork(sd, ad, cfg[\"hidden_dim\"]).to(self.device)\n",
    "        self.target_net = QNetwork(sd, ad, cfg[\"hidden_dim\"]).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimiser = optim.Adam(self.policy_net.parameters(), lr=cfg[\"lr\"])\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        self._steps = 0\n",
    "        self._target_freq = cfg[\"target_update_freq\"]\n",
    "\n",
    "    def choose_action(self, state, greedy=False):\n",
    "        if not greedy and np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        with torch.no_grad():\n",
    "            t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            return self.policy_net(t).argmax().item()\n",
    "\n",
    "    def store(self, s, a, r, s2, done):\n",
    "        self.buffer.push(s, a, r, s2, done)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None\n",
    "        batch = Transition(*zip(*self.buffer.sample(self.batch_size)))\n",
    "\n",
    "        s = torch.FloatTensor(np.array(batch.state)).to(self.device)\n",
    "        a = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
    "        r = torch.FloatTensor(batch.reward).unsqueeze(1).to(self.device)\n",
    "        s2 = torch.FloatTensor(np.array(batch.next_state)).to(self.device)\n",
    "        d = torch.BoolTensor(batch.done).unsqueeze(1).to(self.device)\n",
    "\n",
    "        q = self.policy_net(s).gather(1, a)\n",
    "        with torch.no_grad():\n",
    "            q2 = self.target_net(s2).max(1)[0].unsqueeze(1)\n",
    "            target = r + self.gamma * q2 * (~d)\n",
    "\n",
    "        loss = self.loss_fn(q, target)\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)\n",
    "        self.optimiser.step()\n",
    "\n",
    "        self._steps += 1\n",
    "        if self._steps % self._target_freq == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        return loss.item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.eps_min, self.epsilon * self.eps_decay)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. Training loop\n",
    "# ============================================================================\n",
    "\n",
    "def train_dqn(env, agent, cfg):\n",
    "    n_ep = cfg[\"n_episodes\"]\n",
    "    warmup = cfg[\"train_start_step\"]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING DQN AGENT\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  Episodes: {n_ep}  |  Steps/ep: {env.max_steps}  \"\n",
    "          f\"|  Warmup: {warmup}\")\n",
    "\n",
    "    ep_rewards, ep_losses = [], []\n",
    "    total_steps = 0\n",
    "\n",
    "    for ep in range(n_ep):\n",
    "        s, _ = env.reset()\n",
    "        ep_r, ep_l, n_upd = 0.0, 0.0, 0\n",
    "\n",
    "        for _ in range(env.max_steps):\n",
    "            a = agent.choose_action(s)\n",
    "            s2, r, done, _, _ = env.step(a)\n",
    "            agent.store(s, a, r, s2, done)\n",
    "\n",
    "            if total_steps > warmup:\n",
    "                loss = agent.update()\n",
    "                if loss is not None:\n",
    "                    ep_l += loss\n",
    "                    n_upd += 1\n",
    "\n",
    "            s = s2\n",
    "            ep_r += r\n",
    "            total_steps += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "        ep_rewards.append(ep_r)\n",
    "        ep_losses.append(ep_l / max(n_upd, 1))\n",
    "\n",
    "        if (ep + 1) % 50 == 0:\n",
    "            avg = np.mean(ep_rewards[-50:])\n",
    "            print(f\"  ep {ep+1:4d}  |  avg reward {avg:8.2f}  |  \"\n",
    "                  f\"eps {agent.epsilon:.4f}\")\n",
    "\n",
    "    print(f\"\\n  Final 50-episode avg reward: \"\n",
    "          f\"{np.mean(ep_rewards[-50:]):.2f}\")\n",
    "    return ep_rewards, ep_losses\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 9. Taylor Rule baseline\n",
    "# ============================================================================\n",
    "\n",
    "def taylor_rule(pi, u, r_star=2.0, pi_star=2.0, u_star=5.0,\n",
    "                a_pi=1.5, a_u=0.5):\n",
    "    \"\"\"Standard Taylor (1993) rule with unemployment gap.\"\"\"\n",
    "    return np.clip(r_star + pi + a_pi * (pi - pi_star)\n",
    "                   + a_u * (u - u_star), 0, 20)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 10. Historical policy generation\n",
    "# ============================================================================\n",
    "\n",
    "def generate_historical_policies(historical, agent, env):\n",
    "    \"\"\"\n",
    "    Walk through the full historical record and record what each policy\n",
    "    (DQN, Taylor Rule) would have recommended at every month.\n",
    "    \"\"\"\n",
    "    df = historical.copy()\n",
    "    df[\"taylor_rate\"] = df.apply(\n",
    "        lambda r: taylor_rule(r[\"inflation\"], r[\"unemployment\"]),\n",
    "        axis=1)\n",
    "    df[\"dqn_rate\"] = np.nan\n",
    "\n",
    "    agent_eps_backup = agent.epsilon\n",
    "    agent.epsilon = 0.0  # greedy\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if i < 30:\n",
    "            continue\n",
    "        row = df.iloc[i]\n",
    "        state = np.array([row[\"inflation\"], row[\"unemployment\"],\n",
    "                          row[\"capacity_util\"], row[\"fed_funds\"]],\n",
    "                         dtype=np.float32)\n",
    "        action = agent.choose_action(state, greedy=True)\n",
    "        df.iloc[i, df.columns.get_loc(\"dqn_rate\")] = env.rate_grid[action]\n",
    "\n",
    "    agent.epsilon = agent_eps_backup\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 11. Visualisation\n",
    "# ============================================================================\n",
    "\n",
    "# Palette\n",
    "C1, C2, C3 = \"#2563eb\", \"#dc2626\", \"#7c3aed\"   # blue, red, purple\n",
    "C_BG = \"#fafafa\"\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\": C_BG, \"axes.facecolor\": C_BG,\n",
    "    \"axes.grid\": True, \"grid.color\": \"#e5e7eb\", \"grid.linewidth\": 0.5,\n",
    "    \"font.size\": 11, \"axes.spines.top\": False, \"axes.spines.right\": False,\n",
    "})\n",
    "\n",
    "RECESSIONS = [\n",
    "    (\"1980-01\", \"1982-11\"), (\"1990-07\", \"1991-03\"),\n",
    "    (\"2001-03\", \"2001-11\"), (\"2007-12\", \"2009-06\"),\n",
    "    (\"2020-02\", \"2020-04\"),\n",
    "]\n",
    "\n",
    "\n",
    "def plot_data_overview(hist, save=None, dpi=200):\n",
    "    \"\"\"Six-panel overview of the raw economic indicators.\"\"\"\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 11))\n",
    "    fig.suptitle(\"US Economic Indicators (Monthly, 1973–2025)\",\n",
    "                 fontweight=\"bold\", fontsize=14, y=0.995)\n",
    "\n",
    "    panels = [\n",
    "        (\"inflation\", \"Inflation (YoY %)\", C1, {\"hline\": 2.0, \"hl\": \"Target\"}),\n",
    "        (\"unemployment\", \"Unemployment rate (%)\", C2, {\"hline\": 5.0, \"hl\": \"Natural rate\"}),\n",
    "        (\"capacity_util\", \"Capacity utilisation (%)\", \"#c2410c\", {\"hline\": 100, \"hl\": \"Full capacity\"}),\n",
    "        ([\"fed_funds\", \"treasury_10y\"], \"Interest rates (%)\", None, {}),\n",
    "        (\"term_spread\", \"Term spread (10Y − FFR, pp)\", \"#7c3aed\", {\"hline\": 0}),\n",
    "        (\"fin_conditions\", \"Financial Conditions Index\", \"#c2410c\", {\"hline\": 0, \"hl\": \"Neutral\"}),\n",
    "    ]\n",
    "\n",
    "    for ax, (col, title, color, opts) in zip(axes.flat, panels):\n",
    "        if isinstance(col, list):\n",
    "            ax.plot(hist.index, hist[col[0]], lw=1.5, color=C1, label=\"Fed funds\")\n",
    "            ax.plot(hist.index, hist[col[1]], lw=1.5, color=C2,\n",
    "                    alpha=0.7, label=\"10Y Treasury\")\n",
    "            ax.legend(fontsize=9, frameon=False)\n",
    "        else:\n",
    "            ax.plot(hist.index, hist[col], lw=1.5, color=color)\n",
    "        if \"hline\" in opts:\n",
    "            ax.axhline(opts[\"hline\"], color=\"grey\", ls=\"--\", lw=1, alpha=0.6,\n",
    "                       label=opts.get(\"hl\"))\n",
    "            if \"hl\" in opts:\n",
    "                ax.legend(fontsize=9, frameon=False)\n",
    "        ax.set_title(title, fontweight=\"bold\", fontsize=11)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if save:\n",
    "        fig.savefig(save, dpi=dpi, bbox_inches=\"tight\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_cv_results(cv_results, save=None, dpi=200):\n",
    "    \"\"\"Bar chart of cross-validation MSE by specification and fold.\"\"\"\n",
    "    specs = list(cv_results.keys())\n",
    "    folds = [f[\"fold\"] for f in cv_results[specs[0]][\"folds\"]]\n",
    "    n_folds = len(folds)\n",
    "    n_specs = len(specs)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(13, 5.5))\n",
    "    x = np.arange(n_folds)\n",
    "    width = 0.8 / n_specs\n",
    "    colors = [C1, C2, C3, \"#f59e0b\"]  # blue, red, purple, amber\n",
    "\n",
    "    for i, spec in enumerate(specs):\n",
    "        mses = [f[\"mse\"] for f in cv_results[spec][\"folds\"]]\n",
    "        ax.bar(x + i * width, mses, width, label=spec,\n",
    "               color=colors[i % len(colors)], alpha=0.7,\n",
    "               edgecolor=\"white\", lw=1)\n",
    "\n",
    "    ax.set_xticks(x + width * (n_specs - 1) / 2)\n",
    "    ax.set_xticklabels(folds, fontsize=10)\n",
    "    ax.set_ylabel(\"Test MSE\")\n",
    "    ax.set_title(\"Time-series CV: forecast error by specification and period\",\n",
    "                 fontweight=\"bold\")\n",
    "    ax.legend(frameon=False, fontsize=9)\n",
    "    fig.tight_layout()\n",
    "    if save:\n",
    "        fig.savefig(save, dpi=dpi, bbox_inches=\"tight\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_training(rewards, losses, save=None, dpi=200):\n",
    "    \"\"\"Training reward and loss curves.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 7.5))\n",
    "    w = 30\n",
    "\n",
    "    ax1.plot(rewards, alpha=0.2, color=C1, lw=0.8)\n",
    "    if len(rewards) >= w:\n",
    "        sm = np.convolve(rewards, np.ones(w) / w, mode=\"valid\")\n",
    "        ax1.plot(range(w - 1, len(rewards)), sm, color=C1, lw=2.5,\n",
    "                 label=f\"{w}-episode avg\")\n",
    "    ax1.set_ylabel(\"Episode reward\")\n",
    "    ax1.set_title(\"Training: cumulative reward per episode\",\n",
    "                  fontweight=\"bold\")\n",
    "    ax1.legend(frameon=False)\n",
    "\n",
    "    valid = [(i, l) for i, l in enumerate(losses) if l and l > 0]\n",
    "    if valid and len(valid) >= w:\n",
    "        ix, vals = zip(*valid)\n",
    "        sm = np.convolve(vals, np.ones(w) / w, mode=\"valid\")\n",
    "        ax2.plot(range(ix[0] + w - 1, ix[0] + w - 1 + len(sm)),\n",
    "                 sm, color=C2, lw=2.5)\n",
    "    ax2.set_ylabel(\"TD loss\")\n",
    "    ax2.set_xlabel(\"Episode\")\n",
    "    ax2.set_title(\"Training: Huber loss\", fontweight=\"bold\")\n",
    "\n",
    "    fig.tight_layout(h_pad=2.5)\n",
    "    if save:\n",
    "        fig.savefig(save, dpi=dpi, bbox_inches=\"tight\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_policy_comparison(df, save=None, dpi=200):\n",
    "    \"\"\"Historical interest-rate comparison + deviation panel.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 9), sharex=True)\n",
    "    fig.suptitle(\"Monetary policy comparison: DQN vs Taylor Rule vs \"\n",
    "                 \"Federal Reserve (1975–2025)\",\n",
    "                 fontweight=\"bold\", fontsize=13, y=0.995)\n",
    "\n",
    "    # Rates\n",
    "    axes[0].plot(df.index, df[\"fed_funds\"], lw=2, color=C1,\n",
    "                 label=\"Federal Reserve (actual)\", alpha=0.9)\n",
    "    axes[0].plot(df.index, df[\"taylor_rate\"], lw=1.8, color=C2,\n",
    "                 ls=\"--\", label=\"Taylor Rule\", alpha=0.8)\n",
    "    axes[0].plot(df.index, df[\"dqn_rate\"], lw=2, color=C3,\n",
    "                 ls=\":\", label=\"DQN agent\", alpha=0.85)\n",
    "    for s, e in RECESSIONS:\n",
    "        axes[0].axvspan(s, e, alpha=0.12, color=\"#fca5a5\", zorder=0)\n",
    "    axes[0].set_ylabel(\"Nominal interest rate (%)\")\n",
    "    axes[0].set_ylim(-1, 22)\n",
    "    axes[0].legend(loc=\"upper right\", frameon=True, fontsize=10)\n",
    "    axes[0].set_title(\"Interest-rate policies\", fontweight=\"bold\")\n",
    "\n",
    "    # Deviations\n",
    "    t_dev = df[\"taylor_rate\"] - df[\"fed_funds\"]\n",
    "    d_dev = df[\"dqn_rate\"] - df[\"fed_funds\"]\n",
    "    axes[1].fill_between(df.index, t_dev, 0, alpha=0.15, color=C2)\n",
    "    axes[1].fill_between(df.index, d_dev, 0, alpha=0.15, color=C3)\n",
    "    axes[1].plot(df.index, t_dev, lw=1.5, color=C2,\n",
    "                 label=\"Taylor deviation\", alpha=0.8)\n",
    "    axes[1].plot(df.index, d_dev, lw=1.5, color=C3,\n",
    "                 label=\"DQN deviation\", alpha=0.8)\n",
    "    axes[1].axhline(0, color=C1, lw=1, alpha=0.4)\n",
    "    axes[1].set_ylabel(\"Deviation from Fed rate (pp)\")\n",
    "    axes[1].set_xlabel(\"Year\")\n",
    "    axes[1].legend(loc=\"upper right\", frameon=True, fontsize=10)\n",
    "    axes[1].set_title(\"Policy deviations from actual Federal Reserve decisions\",\n",
    "                      fontweight=\"bold\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if save:\n",
    "        fig.savefig(save, dpi=dpi, bbox_inches=\"tight\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def compute_deviation_metrics(df):\n",
    "    \"\"\"Compute and print MAD / RMSE of each policy vs the Fed.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DEVIATION FROM ACTUAL FED DECISIONS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for name, col in [(\"Taylor Rule\", \"taylor_rate\"),\n",
    "                      (\"DQN Agent\", \"dqn_rate\")]:\n",
    "        dev = (df[col] - df[\"fed_funds\"]).dropna()\n",
    "        mad = np.abs(dev).mean()\n",
    "        rmse = np.sqrt((dev ** 2).mean())\n",
    "        print(f\"  {name:15s}  MAD = {mad:.3f} pp  |  RMSE = {rmse:.3f} pp\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 12. Main\n",
    "# ============================================================================\n",
    "\n",
    "def main(cfg=None):\n",
    "    cfg = cfg or DEFAULT_CFG\n",
    "    os.makedirs(cfg[\"output_path\"], exist_ok=True)\n",
    "    out = cfg[\"output_path\"]\n",
    "    dpi = cfg[\"fig_dpi\"]\n",
    "\n",
    "    # --- Data ---\n",
    "    data = load_data(cfg[\"data_path\"])\n",
    "    historical = engineer_features(data, cfg[\"unemployment_natural\"])\n",
    "\n",
    "    # --- Descriptive plots ---\n",
    "    plot_data_overview(historical,\n",
    "                       save=f\"{out}/data_overview.png\", dpi=dpi)\n",
    "\n",
    "    # --- Cross-validation ---\n",
    "    cv_results, best_spec = time_series_cv(historical, cfg[\"cv_folds\"])\n",
    "    plot_cv_results(cv_results,\n",
    "                    save=f\"{out}/cv_results.png\", dpi=dpi)\n",
    "\n",
    "    # --- Final model ---\n",
    "    model, scaler, feats = train_final_model(historical, best_spec)\n",
    "\n",
    "    # --- Environment + agent ---\n",
    "    env = MonetaryPolicyEnv(model, scaler, feats, historical, cfg)\n",
    "    agent = DQNAgent(env, cfg)\n",
    "\n",
    "    # --- Train ---\n",
    "    ep_rewards, ep_losses = train_dqn(env, agent, cfg)\n",
    "    plot_training(ep_rewards, ep_losses,\n",
    "                  save=f\"{out}/training_curves.png\", dpi=dpi)\n",
    "\n",
    "    # --- Historical policy comparison ---\n",
    "    df = generate_historical_policies(historical, agent, env)\n",
    "    plot_policy_comparison(df,\n",
    "                           save=f\"{out}/policy_comparison.png\", dpi=dpi)\n",
    "    compute_deviation_metrics(df)\n",
    "\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"All outputs saved to {out}/\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return {\n",
    "        \"historical\": df, \"env\": env, \"agent\": agent,\n",
    "        \"cv_results\": cv_results, \"best_spec\": best_spec,\n",
    "        \"episode_rewards\": ep_rewards,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Entry point\n",
    "# ============================================================================\n",
    "\n",
    "def _in_notebook():\n",
    "    \"\"\"Detect if we're running inside a Jupyter/IPython notebook.\"\"\"\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        return get_ipython() is not None\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if _in_notebook():\n",
    "        # Running in Jupyter — edit paths here directly\n",
    "        cfg = {\n",
    "            **DEFAULT_CFG,\n",
    "            \"data_path\": \"/Users/leoss/Downloads\",\n",
    "            \"output_path\": \"/Users/leoss/Desktop/Portfolio/Website-/Central bank/Outputs\",\n",
    "        }\n",
    "        results = main(cfg)\n",
    "    else:\n",
    "        # Running from terminal — use CLI arguments\n",
    "        parser = argparse.ArgumentParser(\n",
    "            description=\"RL for Monetary Policy Optimisation\")\n",
    "        parser.add_argument(\"--data\", default=DEFAULT_CFG[\"data_path\"])\n",
    "        parser.add_argument(\"--output\", default=DEFAULT_CFG[\"output_path\"])\n",
    "        parser.add_argument(\"--episodes\", type=int,\n",
    "                            default=DEFAULT_CFG[\"n_episodes\"])\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        cfg = {**DEFAULT_CFG, \"data_path\": args.data,\n",
    "               \"output_path\": args.output, \"n_episodes\": args.episodes}\n",
    "        main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c2577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
